{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab02cfaa",
   "metadata": {},
   "source": [
    "\n",
    "# Whisper Keyboard Event LoRA Fine-Tuning Notebook\n",
    "\n",
    "This notebook translates the updated `spec.md` into executable code that relies on Hugging Face's `transformers` stack for Whisper fine-tuning with LoRA. Every section mirrors the spec: tokenizer creation, processor wiring, dataset preparation, and a minimal training loop that exercises `Seq2SeqTrainer` with LoRA-enabled encoder layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec9977",
   "metadata": {},
   "source": [
    "\n",
    "The workflow follows these stages:\n",
    "\n",
    "1. Define the keyboard vocabulary exactly as specified and implement a `PreTrainedTokenizer` subclass for keyboard events.\n",
    "2. Bind the tokenizer to a `WhisperProcessor` so data pipelines reuse `transformers` utilities end-to-end.\n",
    "3. Provide a dataset implementation that loads (or, for testing, injects) audio/event pairs and emits tensors expected by Whisper.\n",
    "4. Configure a lightweight Whisper model, wrap it with `peft` LoRA adapters on encoder attention projections, and fine-tune using `Seq2SeqTrainer`.\n",
    "5. Verify the pipeline by running a tiny synthetic training/eval cycle and decoding generated event sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c43a46-ef0f-4a6a-8862-1ab8b0877c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "\n",
    "import librosa\n",
    "import types\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import requests\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    PreTrainedTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "\n",
    "# Deterministic behavior keeps the demonstration reproducible.\n",
    "SEED = 7\n",
    "SAMPLING_RATE = 16000\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56425af-7d8a-4ca5-93d8-dd61b6932867",
   "metadata": {},
   "source": [
    "## Fetch the Dataset\n",
    "The set of recordings along with their key press data is stored at kasca-esque.fly.dev/recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd604b50-9937-4650-9e3c-159d17b70bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://kasca-esque.fly.dev/recordings\"\n",
    "SAVE_DIR = Path(\"/home/matty/Projects/kasca-esque/finetune/recordings\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(SAVE_DIR)\n",
    "\n",
    "response = requests.get(BASE_URL, timeout=30)\n",
    "resp.raise_for_status()\n",
    "manifest = resp.json()\n",
    "print(manifest)\n",
    "\n",
    "def download_file(relative_path: str) -> Path:\n",
    "    url = f\"{BASE_URL}/{relative_path.lstrip('/')}\"\n",
    "    local_path = SAVE_DIR / relative_path\n",
    "    r = requests.get(url, stream=True)\n",
    "    r.raise_for_status()\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return local_path\n",
    "\n",
    "downloaded = [{\"audio\": download_file(entry[\"audio\"]), \"keylog\": download_file(entry[\"keylog\"])} for entry in manifest]\n",
    "print(f\"Downloaded {len(downloaded)} recordings into {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1a7ab",
   "metadata": {},
   "source": [
    "\n",
    "## Keyboard Vocabulary\n",
    "\n",
    "The tokenizer reuses the spec's `KeyboardEvent.code` values and the two event types (`down`, `up`). This cell simply materializes the lists so the tokenizer can reference them without any ad-hoc logic elsewhere in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cad3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total physical keys: 83\n",
      "Total tokens including events and specials: 168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LETTER_CODES = [f\"Key{chr(i)}\" for i in range(ord('A'), ord('Z') + 1)]\n",
    "DIGIT_CODES = [f\"Digit{i}\" for i in range(10)]\n",
    "PUNCTUATION_CODES = [\n",
    "    'Minus', 'Equal', 'BracketLeft', 'BracketRight', 'Backslash',\n",
    "    'Semicolon', 'Quote', 'Backquote', 'Comma', 'Period', 'Slash'\n",
    "]\n",
    "MODIFIER_CODES = [\n",
    "    'ShiftLeft', 'ShiftRight', 'ControlLeft', 'ControlRight',\n",
    "    'AltLeft', 'AltRight', 'MetaLeft', 'MetaRight', 'CapsLock'\n",
    "]\n",
    "WHITESPACE_EDIT_CODES = ['Space', 'Tab', 'Enter', 'Backspace', 'Delete']\n",
    "NAVIGATION_CODES = [\n",
    "    'ArrowLeft', 'ArrowRight', 'ArrowUp', 'ArrowDown',\n",
    "    'Home', 'End', 'PageUp', 'PageDown', 'Insert', 'Escape'\n",
    "]\n",
    "FUNCTION_CODES = [f\"F{i}\" for i in range(1, 13)]\n",
    "\n",
    "PHYSICAL_KEY_CODES = (\n",
    "    LETTER_CODES\n",
    "    + DIGIT_CODES\n",
    "    + PUNCTUATION_CODES\n",
    "    + MODIFIER_CODES\n",
    "    + WHITESPACE_EDIT_CODES\n",
    "    + NAVIGATION_CODES\n",
    "    + FUNCTION_CODES\n",
    ")\n",
    "EVENT_TYPES = ['down', 'up']\n",
    "SPECIAL_TOKENS = ['<BOS>', '<EOS>']\n",
    "\n",
    "print(f\"Total physical keys: {len(PHYSICAL_KEY_CODES)}\")\n",
    "print(f\"Total tokens including events and specials: {len(PHYSICAL_KEY_CODES) * len(EVENT_TYPES) + len(SPECIAL_TOKENS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d72b8e",
   "metadata": {},
   "source": [
    "\n",
    "## KeyboardEventTokenizer\n",
    "\n",
    "The tokenizer subclasses `PreTrainedTokenizer` so that it can be bundled into a `WhisperProcessor`. Custom helper methods (`encode_events` / `decode_events`) work directly with keyboard event dictionaries, while the inherited text APIs still function (by treating each event token as whitespace-delimited text) to maintain compatibility with `transformers` utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8e72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeyboardEventTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"Tokenizer that maps keyboard event dictionaries to token ids.\n",
    "\n",
    "    The implementation leans entirely on `PreTrainedTokenizer` hooks so that\n",
    "    Hugging Face processors/trainers can treat it like any other tokenizer.\n",
    "    Custom encode/decode helpers keep higher-level code ergonomic.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = {\"vocab_file\": \"keyboard_vocab.json\"}\n",
    "    model_input_names = [\"input_ids\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        codes: Sequence[str] = PHYSICAL_KEY_CODES,\n",
    "        event_types: Sequence[str] = EVENT_TYPES,\n",
    "        special_tokens: Sequence[str] = SPECIAL_TOKENS,\n",
    "        key_to_code: Optional[Dict[str, str]] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.codes = list(codes)\n",
    "        self.event_types = list(event_types)\n",
    "        self.special_tokens = list(special_tokens)\n",
    "        self.key_to_code = key_to_code or {}\n",
    "\n",
    "        vocab = {}\n",
    "        for token in self.special_tokens:\n",
    "            vocab[token] = len(vocab)\n",
    "        for code in self.codes:\n",
    "            for event_type in self.event_types:\n",
    "                vocab[f\"{code}_{event_type}\"] = len(vocab)\n",
    "\n",
    "        self._vocab = vocab\n",
    "        self._id_to_token = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=self.special_tokens[0],\n",
    "            eos_token=self.special_tokens[1],\n",
    "            pad_token=self.special_tokens[1],  # Spec only defines BOS/EOS, so reuse EOS for padding.\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:  # type: ignore[override]\n",
    "        return len(self._vocab)\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:  # type: ignore[override]\n",
    "        return dict(self._vocab, **self.added_tokens_encoder)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:  # type: ignore[override]\n",
    "        return text.strip().split()\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:  # type: ignore[override]\n",
    "        return self._vocab.get(token, self.eos_token_id)\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:  # type: ignore[override]\n",
    "        return self._id_to_token.get(index, self.eos_token)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]:  # type: ignore[override]\n",
    "        if token_ids_1 is not None:\n",
    "            raise ValueError(\"This tokenizer does not support pair encodings.\")\n",
    "        return [self.bos_token_id] + token_ids + [self.eos_token_id]\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None):  # type: ignore[override]\n",
    "        vocab_file = os.path.join(save_directory, (filename_prefix + '-' if filename_prefix else '') + self.vocab_files_names['vocab_file'])\n",
    "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self._vocab, f, indent=2)\n",
    "        return (vocab_file,)\n",
    "\n",
    "    # --- Custom helpers for keyboard events ---------------------------------------------------\n",
    "\n",
    "    def encode_events(self, events: Sequence[Dict[str, str]]) -> List[int]:\n",
    "        token_ids = [self.bos_token_id]\n",
    "        for event in events:\n",
    "            code = event.get('code') or self.key_to_code.get(event.get('key', ''))\n",
    "            if not code:\n",
    "                continue\n",
    "            event_type = event['event_type']\n",
    "            normalized_type = event_type.replace('key', '')\n",
    "            token = f\"{code}_{normalized_type}\"\n",
    "            if token in self._vocab:\n",
    "                token_ids.append(self._vocab[token])\n",
    "        token_ids.append(self.eos_token_id)\n",
    "        return token_ids\n",
    "\n",
    "    def decode_events(self, token_ids: Sequence[int]) -> List[Dict[str, str]]:\n",
    "        events = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in (self.bos_token_id, self.eos_token_id):\n",
    "                continue\n",
    "            token = self._id_to_token.get(token_id)\n",
    "            if not token or '_' not in token:\n",
    "                continue\n",
    "            code, event_type = token.rsplit('_', 1)\n",
    "            events.append({'code': code, 'event_type': f'key{event_type}'})\n",
    "        return events\n",
    "\n",
    "import transformers as _transformers_module\n",
    "setattr(_transformers_module, 'KeyboardEventTokenizer', KeyboardEventTokenizer)\n",
    "WhisperProcessor.tokenizer_class = ('KeyboardEventTokenizer', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f66de7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 168\n",
      "Encoded IDs: [0, 2, 3, 118, 119, 1]\n",
      "Decoded events: [{'code': 'KeyA', 'event_type': 'keydown'}, {'code': 'KeyA', 'event_type': 'keyup'}, {'code': 'Enter', 'event_type': 'keydown'}, {'code': 'Enter', 'event_type': 'keyup'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = KeyboardEventTokenizer()\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "sample_events = [\n",
    "    {'code': 'KeyA', 'event_type': 'keydown'},\n",
    "    {'code': 'KeyA', 'event_type': 'keyup'},\n",
    "    {'code': 'Enter', 'event_type': 'keydown'},\n",
    "    {'code': 'Enter', 'event_type': 'keyup'},\n",
    "]\n",
    "encoded = tokenizer.encode_events(sample_events)\n",
    "decoded = tokenizer.decode_events(encoded)\n",
    "print('Encoded IDs:', encoded)\n",
    "print('Decoded events:', decoded)\n",
    "assert len(encoded) == len(decoded) + 2  # accounting for BOS/EOS\n",
    "assert decoded == sample_events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d595f8b",
   "metadata": {},
   "source": [
    "\n",
    "## Whisper Processor\n",
    "\n",
    "This cell wires the tokenizer into a `WhisperProcessor` so that feature extraction and tokenization stay centralized inside `transformers`. The processor is saved and reloaded to prove the tokenizer integrates cleanly with standard serialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d1cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor produced input shape: torch.Size([1, 80, 3000])\n"
     ]
    }
   ],
   "source": [
    "artifacts_dir = Path('artifacts')\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor(\n",
    "    feature_size=80,\n",
    "    sampling_rate=SAMPLING_RATE,\n",
    "    chunk_length=30,\n",
    "    hop_length=160,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Sanity check: process a short silence clip to verify the processor pipeline.\n",
    "zero_audio = np.zeros(SAMPLING_RATE, dtype=np.float32)\n",
    "processed = processor(audio=zero_audio, sampling_rate=SAMPLING_RATE, return_tensors='pt')\n",
    "print('Processor produced input shape:', processed.input_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13c108",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset utilities\n",
    "\n",
    "The dataset matches the spec: it pairs `.webm` audio files with `.json` keystroke annotations when a recordings directory is provided. For reproducible testing in this notebook, we also allow injecting synthetic audio/event examples so the machinery can run end-to-end without external data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0084d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class RecordingExample:\n",
    "    audio: np.ndarray\n",
    "    sampling_rate: int\n",
    "    events: List[Dict[str, str]]\n",
    "\n",
    "\n",
    "class KeyboardEventDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        processor: WhisperProcessor,\n",
    "        recordings_dir: Optional[Path] = None,\n",
    "        max_length: int = 1024,\n",
    "        examples: Optional[List[RecordingExample]] = None,\n",
    "    ) -> None:\n",
    "        self.processor = processor\n",
    "        self.recordings_dir = Path(recordings_dir) if recordings_dir else None\n",
    "        self.max_length = max_length\n",
    "        self.examples = examples\n",
    "        self.pairs = self._find_pairs() if self.examples is None else []\n",
    "\n",
    "    def _find_pairs(self) -> List[Dict[str, Path]]:\n",
    "        if not self.recordings_dir:\n",
    "            return []\n",
    "        pairs = []\n",
    "        for json_path in self.recordings_dir.glob('*.json'):\n",
    "            if 'DELETED' in json_path.name:\n",
    "                continue\n",
    "            webm_path = json_path.with_suffix('.webm')\n",
    "            if webm_path.exists():\n",
    "                pairs.append({'audio_path': webm_path, 'json_path': json_path})\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.examples is not None:\n",
    "            return len(self.examples)\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def _load_example_from_disk(self, idx: int) -> RecordingExample:\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        waveform, sampling_rate = torchaudio.load(pair[\"audio_path\"])\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        if sampling_rate != SAMPLING_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=SAMPLING_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "            sampling_rate = SAMPLING_RATE\n",
    "        audio = waveform.squeeze(0).numpy()\n",
    "        \n",
    "        with open(pair['json_path'], 'r', encoding='utf-8') as f:\n",
    "            events = json.load(f)['keystrokes']\n",
    "        return RecordingExample(audio=audio, sampling_rate=SAMPLING_RATE, events=events)\n",
    "\n",
    "    def _prepare_features(self, audio: np.ndarray, sampling_rate: int) -> torch.Tensor:\n",
    "        if sampling_rate != SAMPLING_RATE:\n",
    "            audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=SAMPLING_RATE)\n",
    "        features = self.processor.feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=SAMPLING_RATE,\n",
    "            return_tensors='pt'\n",
    "        ).input_features[0]\n",
    "        return features\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        example = self.examples[idx] if self.examples is not None else self._load_example_from_disk(idx)\n",
    "        features = self._prepare_features(example.audio, example.sampling_rate)\n",
    "        token_ids = self.processor.tokenizer.encode_events(example.events)\n",
    "        token_ids = token_ids[:self.max_length]\n",
    "        labels = torch.tensor(token_ids, dtype=torch.long)\n",
    "        return {\n",
    "            'input_features': features,\n",
    "            'labels': labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a7a74",
   "metadata": {},
   "source": [
    "\n",
    "### Synthetic data for testing\n",
    "\n",
    "To keep the notebook self-contained (and runnable without external recordings), the next cell generates simple sine-wave audio clips with random keyboard events, then exercises the dataset implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature shape: torch.Size([80, 3000])\n",
      "Label ids: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_sine_wave(duration_s: float = 0.4, sr: int = SAMPLING_RATE, freq: float = 220.0) -> np.ndarray:\n",
    "    t = np.linspace(0, duration_s, int(sr * duration_s), endpoint=False)\n",
    "    return 0.05 * np.sin(2 * math.pi * freq * t)\n",
    "\n",
    "\n",
    "def random_events(num_events: int = 6) -> List[Dict[str, str]]:\n",
    "    events: List[Dict[str, str]] = []\n",
    "    for _ in range(num_events):\n",
    "        code = random.choice(PHYSICAL_KEY_CODES)\n",
    "        event_type = random.choice(['keydown', 'keyup'])\n",
    "        events.append({'code': code, 'event_type': event_type})\n",
    "    return events\n",
    "\n",
    "\n",
    "synthetic_examples = [\n",
    "    RecordingExample(\n",
    "        audio=generate_sine_wave(freq=220 + 20 * i),\n",
    "        sampling_rate=SAMPLING_RATE,\n",
    "        events=random_events(),\n",
    "    )\n",
    "    for i in range(4)\n",
    "]\n",
    "\n",
    "dataset = KeyboardEventDataset(processor=processor, max_length=64, recordings_dir=\"/home/matty/Projects/kasca-esque/recordings\")\n",
    "sample = dataset[0]\n",
    "print('Input feature shape:', sample['input_features'].shape)\n",
    "print('Label ids:', sample['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf1543",
   "metadata": {},
   "source": [
    "\n",
    "## Data Collator\n",
    "\n",
    "`Seq2SeqTrainer` expects batches of tensors with uniform shapes. The collator below pads log-mel features along the time axis and uses the tokenizer's pad token for label padding, keeping everything aligned with `transformers` conventions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa8ed738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([59, 80, 3000])\n",
      "Batch labels shape: torch.Size([59, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collate_keyboard_batch(batch: Sequence[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    input_features = [item['input_features'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    max_feat_len = max(feature.shape[-1] for feature in input_features)\n",
    "    padded_inputs = []\n",
    "    for feature in input_features:\n",
    "        pad_amount = max_feat_len - feature.shape[-1]\n",
    "        padded = torch.nn.functional.pad(feature, (0, pad_amount))\n",
    "        padded_inputs.append(padded)\n",
    "    stacked_inputs = torch.stack(padded_inputs)\n",
    "\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels,\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_features': stacked_inputs,\n",
    "        'labels': padded_labels,\n",
    "    }\n",
    "\n",
    "\n",
    "# batch = collate_keyboard_batch([dataset[i] for i in range(2)])\n",
    "batch = collate_keyboard_batch(dataset)\n",
    "print('Batch input shape:', batch['input_features'].shape)\n",
    "print('Batch labels shape:', batch['labels'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60204609",
   "metadata": {},
   "source": [
    "\n",
    "## Whisper model + LoRA adapters\n",
    "\n",
    "A compact Whisper configuration keeps the demo lightweight. We then attach LoRA adapters (via `peft`) to the encoder's attention projections exactly as prescribed in the spec (query/key/value matrices only). This ensures the decoder remains fully trainable while the encoder benefits from parameter-efficient fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e2ec3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,432 || all params: 414,976 || trainable%: 4.4417\n"
     ]
    }
   ],
   "source": [
    "config = WhisperConfig(\n",
    "    vocab_size=processor.tokenizer.vocab_size,\n",
    "    d_model=64,\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    encoder_attention_heads=4,\n",
    "    decoder_attention_heads=4,\n",
    "    encoder_ffn_dim=256,\n",
    "    decoder_ffn_dim=256,\n",
    "    num_mel_bins=feature_extractor.feature_size,\n",
    "    bos_token_id=processor.tokenizer.bos_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
    "model = WhisperForConditionalGeneration(config)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# Allow Trainer to pass either `input_features` (preferred) or `input_ids` (what text models expect).\n",
    "_original_forward = model.forward\n",
    "\n",
    "def forward_with_feature_alias(self, input_features=None, input_ids=None, inputs_embeds=None, **kwargs):\n",
    "    if input_features is None:\n",
    "        if inputs_embeds is not None:\n",
    "            input_features = inputs_embeds\n",
    "        elif input_ids is not None:\n",
    "            input_features = input_ids\n",
    "    return _original_forward(input_features=input_features, **kwargs)\n",
    "\n",
    "model.forward = types.MethodType(forward_with_feature_alias, model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM',\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(DEVICE)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9995353",
   "metadata": {},
   "source": [
    "\n",
    "## Training with `Seq2SeqTrainer`\n",
    "\n",
    "With the model, processor, dataset, and collator in place, we can leverage Hugging Face's `Seq2SeqTrainer` to handle the training loop, evaluation, scheduling, and generation utilities. The tiny synthetic dataset keeps runtime small while still exercising every component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matty/Projects/kasca-esque/finetune/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='257' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 257/5000 10:06:40 < 188:04:12, 0.01 it/s, Epoch 64/1250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset\n",
    "val_dataset = dataset\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=64,\n",
    "    bos_token_id=processor.tokenizer.bos_token_id,\n",
    "    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "class KeyboardSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        sanitized_inputs = {\n",
    "            key: value for key, value in inputs.items() if key not in ('input_ids', 'inputs_embeds')\n",
    "        }\n",
    "        sanitized_inputs = self._prepare_inputs(sanitized_inputs)\n",
    "        outputs = model(**sanitized_inputs)\n",
    "        loss = outputs.loss if hasattr(outputs, 'loss') and outputs.loss is not None else outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(artifacts_dir / 'checkpoints'),\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    eval_strategy='steps',\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    save_strategy='no',\n",
    "    gradient_accumulation_steps=1,\n",
    "    report_to=[],\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    remove_unused_columns=False,\n",
    "    metric_for_best_model=\"wer\",\n",
    ")\n",
    "\n",
    "trainer = KeyboardSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_keyboard_batch,\n",
    ")\n",
    "print(len(train_dataset))\n",
    "trainer.model.generation_config = generation_config\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276a7c3",
   "metadata": {},
   "source": [
    "\n",
    "## Quick generation sanity check\n",
    "\n",
    "Finally, run `model.generate` on one synthetic sample to ensure decoding works through the tokenizer and the output maintains the expected structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894aab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_294646/4183554619.py:41: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(pair['audio_path'], sr=16000)\n",
      "/home/matty/Projects/kasca-esque/finetune/.venv/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded events: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = collate_keyboard_batch([val_dataset[0]])\n",
    "    input_features = batch['input_features'].to(DEVICE)\n",
    "    generated_ids = model.generate(\n",
    "        input_features=input_features,\n",
    "        max_length=32,\n",
    "        bos_token_id=processor.tokenizer.bos_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    )[0].cpu().tolist()\n",
    "\n",
    "print('Generated token ids:', generated_ids)\n",
    "print('Decoded events:', processor.tokenizer.decode_events(generated_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ce50f",
   "metadata": {},
   "source": [
    "\n",
    "The notebook now mirrors the updated spec: it mandates the `transformers` stack for Whisper, uses `WhisperProcessor` + `WhisperForConditionalGeneration`, applies LoRA via `peft`, drives training with `Seq2SeqTrainer`, and validates the custom tokenizer before entering the training loop.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
