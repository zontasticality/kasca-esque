{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper LoRA Keyboard Event Detection\n",
    "\n",
    "This notebook implements fine-tuning of Whisper with LoRA for keyboard event detection from audio.\n",
    "\n",
    "## Overview\n",
    "- **Encoder**: Whisper with LoRA (frozen base weights)\n",
    "- **Decoder**: Custom autoregressive transformer for keyboard events\n",
    "- **Tokenization**: Based on KeyboardEvent.code (physical keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft datasets librosa soundfile tensorboard accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    WhisperModel,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keyboard Event Tokenizer\n",
    "\n",
    "Implements tokenization based on JavaScript KeyboardEvent.code values.\n",
    "This maps physical keys (e.g., KeyA, Digit1) to token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyboardEventTokenizer:\n",
    "    \"\"\"Tokenizer for keyboard events using KeyboardEvent.code values.\n",
    "    \n",
    "    Maps physical keyboard keys to tokens, with separate tokens for keydown/keyup.\n",
    "    Example: 'a' and 'A' both map to KeyA, '1' and '!' both map to Digit1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.event_types = ['down', 'up']\n",
    "        self.special_tokens = ['<BOS>', '<EOS>']\n",
    "\n",
    "        # Physical key codes (KeyboardEvent.code)\n",
    "        self.codes = [\n",
    "            # Letters (26)\n",
    "            *[f'Key{chr(i)}' for i in range(ord('A'), ord('Z') + 1)],\n",
    "            # Digits (10)\n",
    "            *[f'Digit{i}' for i in range(10)],\n",
    "            # Punctuation (11)\n",
    "            'Minus', 'Equal', 'BracketLeft', 'BracketRight', 'Backslash',\n",
    "            'Semicolon', 'Quote', 'Backquote', 'Comma', 'Period', 'Slash',\n",
    "            # Modifiers (9)\n",
    "            'ShiftLeft', 'ShiftRight', 'ControlLeft', 'ControlRight',\n",
    "            'AltLeft', 'AltRight', 'MetaLeft', 'MetaRight', 'CapsLock',\n",
    "            # Whitespace & Editing (5)\n",
    "            'Space', 'Tab', 'Enter', 'Backspace', 'Delete',\n",
    "            # Navigation (10)\n",
    "            'ArrowLeft', 'ArrowRight', 'ArrowUp', 'ArrowDown',\n",
    "            'Home', 'End', 'PageUp', 'PageDown', 'Insert', 'Escape',\n",
    "            # Function Keys (12)\n",
    "            *[f'F{i}' for i in range(1, 13)],\n",
    "        ]\n",
    "\n",
    "        # Mapping from event.key to event.code for legacy dataset compatibility\n",
    "        self.key_to_code = {\n",
    "            # Letters - lowercase map to KeyX\n",
    "            **{chr(i): f'Key{chr(i).upper()}' for i in range(ord('a'), ord('z') + 1)},\n",
    "            # Letters - uppercase map to KeyX (same as lowercase)\n",
    "            **{chr(i): f'Key{chr(i)}' for i in range(ord('A'), ord('Z') + 1)},\n",
    "            # Digits and their shifted symbols\n",
    "            '0': 'Digit0', ')': 'Digit0',\n",
    "            '1': 'Digit1', '!': 'Digit1',\n",
    "            '2': 'Digit2', '@': 'Digit2',\n",
    "            '3': 'Digit3', '#': 'Digit3',\n",
    "            '4': 'Digit4', '$': 'Digit4',\n",
    "            '5': 'Digit5', '%': 'Digit5',\n",
    "            '6': 'Digit6', '^': 'Digit6',\n",
    "            '7': 'Digit7', '&': 'Digit7',\n",
    "            '8': 'Digit8', '*': 'Digit8',\n",
    "            '9': 'Digit9', '(': 'Digit9',\n",
    "            # Punctuation\n",
    "            '-': 'Minus', '_': 'Minus',\n",
    "            '=': 'Equal', '+': 'Equal',\n",
    "            '[': 'BracketLeft', '{': 'BracketLeft',\n",
    "            ']': 'BracketRight', '}': 'BracketRight',\n",
    "            '\\\\': 'Backslash', '|': 'Backslash',\n",
    "            ';': 'Semicolon', ':': 'Semicolon',\n",
    "            \"'\": 'Quote', '\"': 'Quote',\n",
    "            '`': 'Backquote', '~': 'Backquote',\n",
    "            ',': 'Comma', '<': 'Comma',\n",
    "            '.': 'Period', '>': 'Period',\n",
    "            '/': 'Slash', '?': 'Slash',\n",
    "            # Whitespace & Editing\n",
    "            ' ': 'Space',\n",
    "            'Tab': 'Tab',\n",
    "            'Enter': 'Enter',\n",
    "            'Backspace': 'Backspace',\n",
    "            'Delete': 'Delete', 'DELETE': 'Delete',\n",
    "            # Modifiers\n",
    "            'Shift': 'ShiftLeft',  # Generic shift maps to left\n",
    "            'SHIFT_R': 'ShiftRight',\n",
    "            'Control': 'ControlLeft',\n",
    "            'CTRL_L': 'ControlLeft',\n",
    "            'CTRL_R': 'ControlRight',\n",
    "            'Alt': 'AltLeft',\n",
    "            'ALT_GR': 'AltRight',\n",
    "            'CMD': 'MetaLeft',\n",
    "            'Meta': 'MetaLeft',\n",
    "            'CAPS_LOCK': 'CapsLock',\n",
    "            'CapsLock': 'CapsLock',\n",
    "            # Navigation\n",
    "            'ArrowLeft': 'ArrowLeft', 'LEFT': 'ArrowLeft',\n",
    "            'ArrowRight': 'ArrowRight', 'RIGHT': 'ArrowRight',\n",
    "            'ArrowUp': 'ArrowUp', 'UP': 'ArrowUp',\n",
    "            'ArrowDown': 'ArrowDown', 'DOWN': 'ArrowDown',\n",
    "            'Home': 'Home',\n",
    "            'End': 'End',\n",
    "            'PageUp': 'PageUp',\n",
    "            'PageDown': 'PageDown',\n",
    "            'Insert': 'Insert',\n",
    "            'Escape': 'Escape',\n",
    "            # Function Keys\n",
    "            **{f'F{i}': f'F{i}' for i in range(1, 13)},\n",
    "        }\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # Add special tokens\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.vocab[token] = i\n",
    "            self.id_to_token[i] = token\n",
    "\n",
    "        # Add key code event tokens\n",
    "        idx = len(self.special_tokens)\n",
    "        for code in self.codes:\n",
    "            for event_type in self.event_types:\n",
    "                token = f\"{code}_{event_type}\"\n",
    "                self.vocab[token] = idx\n",
    "                self.id_to_token[idx] = token\n",
    "                idx += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.pad_token_id = self.vocab['<BOS>']  # Use BOS as pad for simplicity\n",
    "        self.bos_token_id = self.vocab['<BOS>']\n",
    "        self.eos_token_id = self.vocab['<EOS>']\n",
    "\n",
    "    def encode(self, events: List[Dict]) -> List[int]:\n",
    "        \"\"\"Convert list of key events to token IDs.\n",
    "        \n",
    "        Handles both legacy event.key format and new event.code format.\n",
    "        \n",
    "        Args:\n",
    "            events: List of dicts with 'key' or 'code' and 'event_type'\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs including BOS and EOS\n",
    "        \"\"\"\n",
    "        token_ids = [self.vocab['<BOS>']]\n",
    "        \n",
    "        for event in events:\n",
    "            # Try to get code directly, or map from key\n",
    "            code = event.get('code')\n",
    "            if not code:\n",
    "                key = event.get('key', '')\n",
    "                code = self.key_to_code.get(key)\n",
    "\n",
    "            if code:\n",
    "                event_type = event['event_type'].replace('key', '')  # keydown -> down\n",
    "                token = f\"{code}_{event_type}\"\n",
    "                if token in self.vocab:\n",
    "                    token_ids.append(self.vocab[token])\n",
    "                # Skip unknown codes silently\n",
    "\n",
    "        token_ids.append(self.vocab['<EOS>'])\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> List[Dict]:\n",
    "        \"\"\"Convert token IDs back to key events with code values.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            List of event dicts with 'code' and 'event_type'\n",
    "        \"\"\"\n",
    "        events = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in [0, 1]:  # Skip <BOS> and <EOS>\n",
    "                continue\n",
    "            token = self.id_to_token.get(token_id)\n",
    "            if token and '_' in token:\n",
    "                code, event_type = token.rsplit('_', 1)\n",
    "                events.append({\n",
    "                    'code': code,\n",
    "                    'event_type': f'key{event_type}'\n",
    "                })\n",
    "        return events\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = KeyboardEventTokenizer()\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"\\nNumber of physical keys: {len(tokenizer.codes)}\")\n",
    "print(f\"Number of legacy key mappings: {len(tokenizer.key_to_code)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Tokenizer with Sample Data\n",
    "\n",
    "Load a sample recording and verify tokenization works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample JSON file to test tokenization\n",
    "recordings_dir = '../recordings'\n",
    "sample_files = [f for f in os.listdir(recordings_dir) if f.endswith('.json') and 'DELETED' not in f]\n",
    "\n",
    "print(f\"Found {len(sample_files)} JSON files\")\n",
    "print(f\"Sample file: {sample_files[0]}\")\n",
    "\n",
    "# Load first file\n",
    "with open(os.path.join(recordings_dir, sample_files[0]), 'r') as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "print(f\"\\nTotal events in sample: {len(sample_data['keystrokes'])}\")\n",
    "print(f\"\\nFirst 5 events:\")\n",
    "for i, event in enumerate(sample_data['keystrokes'][:5]):\n",
    "    print(f\"  {i}: {event}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding\n",
    "sample_events = sample_data['keystrokes'][:20]  # First 20 events\n",
    "encoded = tokenizer.encode(sample_events)\n",
    "\n",
    "print(f\"Original events count: {len(sample_events)}\")\n",
    "print(f\"Encoded token count: {len(encoded)} (includes BOS/EOS)\")\n",
    "print(f\"\\nEncoded tokens: {encoded[:10]}...\")\n",
    "\n",
    "# Test decoding\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nDecoded events count: {len(decoded)}\")\n",
    "print(f\"\\nFirst 3 decoded events:\")\n",
    "for i, event in enumerate(decoded[:3]):\n",
    "    print(f\"  {i}: {event}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify round-trip consistency (event types should match)\n",
    "print(\"Verifying round-trip encoding/decoding:\")\n",
    "mismatches = 0\n",
    "for orig, dec in zip(sample_events[:len(decoded)], decoded):\n",
    "    orig_type = orig['event_type']\n",
    "    dec_type = dec['event_type']\n",
    "    if orig_type != dec_type:\n",
    "        print(f\"  Mismatch: {orig} -> {dec}\")\n",
    "        mismatches += 1\n",
    "\n",
    "if mismatches == 0:\n",
    "    print(\"✓ All event types match after round-trip!\")\n",
    "else:\n",
    "    print(f\"✗ Found {mismatches} mismatches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different key variations (uppercase/lowercase, symbols)\n",
    "test_events = [\n",
    "    {'key': 'a', 'event_type': 'keydown'},\n",
    "    {'key': 'A', 'event_type': 'keydown'},\n",
    "    {'key': '1', 'event_type': 'keydown'},\n",
    "    {'key': '!', 'event_type': 'keydown'},\n",
    "    {'key': 'Shift', 'event_type': 'keydown'},\n",
    "    {'key': 'SHIFT_R', 'event_type': 'keydown'},\n",
    "]\n",
    "\n",
    "print(\"Testing key variations:\")\n",
    "for event in test_events:\n",
    "    encoded = tokenizer.encode([event])\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"  {event['key']:10s} -> token {encoded[1]:3d} -> {decoded[0]['code'] if decoded else 'NONE'}\")\n",
    "\n",
    "# Verify 'a' and 'A' map to same code\n",
    "a_lower = tokenizer.encode([{'key': 'a', 'event_type': 'keydown'}])[1]\n",
    "a_upper = tokenizer.encode([{'key': 'A', 'event_type': 'keydown'}])[1]\n",
    "print(f\"\\n'a' and 'A' map to same token: {a_lower == a_upper}\")\n",
    "\n",
    "# Verify '1' and '!' map to same code\n",
    "one_digit = tokenizer.encode([{'key': '1', 'event_type': 'keydown'}])[1]\n",
    "one_symbol = tokenizer.encode([{'key': '!', 'event_type': 'keydown'}])[1]\n",
    "print(f\"'1' and '!' map to same token: {one_digit == one_symbol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Implementation\n",
    "\n",
    "Implements dataset class for loading audio and keyboard event pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyboardEventDataset(Dataset):\n",
    "    \"\"\"Dataset for keyboard event detection from audio.\n",
    "    \n",
    "    Loads paired audio (.webm) and event (.json) files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        recordings_dir: str,\n",
    "        tokenizer: KeyboardEventTokenizer,\n",
    "        feature_extractor: WhisperFeatureExtractor,\n",
    "        max_length: int = 1024,\n",
    "        sample_rate: int = 16000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            recordings_dir: Directory containing .webm and .json files\n",
    "            tokenizer: KeyboardEventTokenizer instance\n",
    "            feature_extractor: Whisper feature extractor\n",
    "            max_length: Maximum sequence length for tokens\n",
    "            sample_rate: Audio sample rate (Whisper expects 16kHz)\n",
    "        \"\"\"\n",
    "        self.recordings_dir = recordings_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Find all audio-json pairs\n",
    "        self.pairs = self._find_pairs()\n",
    "        print(f\"Found {len(self.pairs)} audio-json pairs\")\n",
    "\n",
    "    def _find_pairs(self) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Find matching .webm and .json file pairs.\"\"\"\n",
    "        pairs = []\n",
    "        json_files = glob(f\"{self.recordings_dir}/*.json\")\n",
    "        \n",
    "        for json_path in json_files:\n",
    "            if 'DELETED' in json_path:\n",
    "                continue\n",
    "            base = json_path.replace('.json', '')\n",
    "            webm_path = f\"{base}.webm\"\n",
    "            \n",
    "            if os.path.exists(webm_path):\n",
    "                pairs.append((webm_path, json_path))\n",
    "                \n",
    "        return pairs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Load and process a single audio-event pair.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - input_features: Mel spectrogram features for Whisper encoder\n",
    "                - labels: Token IDs for keyboard events\n",
    "                - attention_mask: Mask for padded tokens\n",
    "        \"\"\"\n",
    "        audio_path, json_path = self.pairs[idx]\n",
    "\n",
    "        # Load audio\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio {audio_path}: {e}\")\n",
    "            # Return empty sample on error\n",
    "            audio = np.zeros(self.sample_rate)\n",
    "\n",
    "        # Extract features (mel spectrogram)\n",
    "        features = self.feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=self.sample_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Load events\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Tokenize events\n",
    "        token_ids = self.tokenizer.encode(data['keystrokes'])\n",
    "        \n",
    "        # Truncate or pad to max_length\n",
    "        if len(token_ids) > self.max_length:\n",
    "            token_ids = token_ids[:self.max_length]\n",
    "        else:\n",
    "            # Pad with BOS token (our pad token)\n",
    "            token_ids = token_ids + [self.tokenizer.pad_token_id] * (self.max_length - len(token_ids))\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1 if tid != self.tokenizer.pad_token_id else 0 for tid in token_ids]\n",
    "\n",
    "        return {\n",
    "            'input_features': features.input_features[0],\n",
    "            'labels': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = KeyboardEventDataset(\n",
    "    recordings_dir=recordings_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=1024,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset loading\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"Sample data shapes:\")\n",
    "print(f\"  input_features: {sample['input_features'].shape}\")\n",
    "print(f\"  labels: {sample['labels'].shape}\")\n",
    "print(f\"  attention_mask: {sample['attention_mask'].shape}\")\n",
    "\n",
    "print(f\"\\nFirst 20 label tokens: {sample['labels'][:20].tolist()}\")\n",
    "print(f\"First 20 attention mask: {sample['attention_mask'][:20].tolist()}\")\n",
    "\n",
    "# Count non-padded tokens\n",
    "non_pad_count = sample['attention_mask'].sum().item()\n",
    "print(f\"\\nNon-padded tokens: {non_pad_count} / {len(sample['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Decoder Architecture\n",
    "\n",
    "Implements a custom transformer decoder for generating keyboard event sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyboardEventDecoder(nn.Module):\n",
    "    \"\"\"Custom transformer decoder for keyboard event prediction.\n",
    "    \n",
    "    Generates autoregressive sequences of keyboard events from Whisper encoder features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 384,  # Match whisper-tiny\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 6,\n",
    "        ff_dim: int = 1536,  # 4x hidden_dim\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_len: int = 1024,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of token vocabulary\n",
    "            hidden_dim: Hidden dimension (should match encoder output)\n",
    "            num_layers: Number of decoder layers\n",
    "            num_heads: Number of attention heads\n",
    "            ff_dim: Feed-forward dimension\n",
    "            dropout: Dropout probability\n",
    "            max_seq_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, max_seq_len, hidden_dim))\n",
    "        \n",
    "        # Transformer decoder layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # Pre-LN\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs [batch, seq_len]\n",
    "            encoder_hidden_states: Encoder outputs [batch, enc_seq_len, hidden_dim]\n",
    "            attention_mask: Mask for padding [batch, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Logits [batch, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Embed tokens\n",
    "        x = self.token_embedding(input_ids)  # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask (prevent attending to future tokens)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        \n",
    "        # Create key padding mask (mask out padding tokens)\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = (attention_mask == 0)  # True for padding positions\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "        \n",
    "        # Apply decoder\n",
    "        x = self.decoder(\n",
    "            tgt=x,\n",
    "            memory=encoder_hidden_states,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=key_padding_mask,\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_proj(x)  # [batch, seq_len, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test decoder initialization\n",
    "decoder = KeyboardEventDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_dim=384,  # whisper-tiny dimension\n",
    "    num_layers=4,\n",
    "    num_heads=6,\n",
    ")\n",
    "\n",
    "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
    "print(f\"Decoder vocab size: {decoder.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Model with Whisper Encoder + LoRA\n",
    "\n",
    "Combines Whisper encoder (with LoRA) and custom decoder into a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperKeyboardModel(nn.Module):\n",
    "    \"\"\"Complete model combining Whisper encoder with LoRA and custom decoder.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        whisper_model_name: str = \"openai/whisper-tiny\",\n",
    "        vocab_size: int = 256,\n",
    "        lora_rank: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained Whisper encoder\n",
    "        print(f\"Loading {whisper_model_name}...\")\n",
    "        whisper = WhisperModel.from_pretrained(whisper_model_name)\n",
    "        self.encoder = whisper.encoder\n",
    "        \n",
    "        # Get encoder config\n",
    "        config = whisper.config\n",
    "        self.hidden_dim = config.d_model\n",
    "        \n",
    "        print(f\"Encoder hidden dim: {self.hidden_dim}\")\n",
    "        \n",
    "        # Apply LoRA to encoder\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # Attention projections\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        print(\"Applying LoRA to encoder...\")\n",
    "        self.encoder = get_peft_model(self.encoder, lora_config)\n",
    "        \n",
    "        # Custom decoder\n",
    "        self.decoder = KeyboardEventDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=4,\n",
    "            num_heads=6,\n",
    "        )\n",
    "        \n",
    "        print(f\"Total model parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        print(f\"Trainable parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_features: Mel spectrogram [batch, n_mels, time]\n",
    "            labels: Target token IDs [batch, seq_len]\n",
    "            attention_mask: Mask for labels [batch, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss, logits\n",
    "        \"\"\"\n",
    "        # Encode audio\n",
    "        encoder_outputs = self.encoder(input_features)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Prepare decoder inputs (shift labels right, start with BOS)\n",
    "        # Input: <BOS> token_1 token_2 ...\n",
    "        # Target: token_1 token_2 ... <EOS>\n",
    "        decoder_input_ids = labels[:, :-1]  # Remove last token\n",
    "        decoder_attention_mask = attention_mask[:, :-1] if attention_mask is not None else None\n",
    "        \n",
    "        # Decode\n",
    "        logits = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        targets = labels[:, 1:]  # Remove BOS token\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            targets.reshape(-1),\n",
    "            ignore_index=0,  # Ignore BOS (pad) token in loss\n",
    "        )\n",
    "        \n",
    "        return loss, logits\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = WhisperKeyboardModel(\n",
    "    whisper_model_name=\"openai/whisper-tiny\",\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with sample data\n",
    "sample_batch = {\n",
    "    'input_features': sample['input_features'].unsqueeze(0).to(device),\n",
    "    'labels': sample['labels'].unsqueeze(0).to(device),\n",
    "    'attention_mask': sample['attention_mask'].unsqueeze(0).to(device),\n",
    "}\n",
    "\n",
    "print(\"Testing forward pass...\")\n",
    "with torch.no_grad():\n",
    "    loss, logits = model(**sample_batch)\n",
    "\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Expected: [batch_size, seq_len-1, vocab_size] = [1, {sample['labels'].shape[0]-1}, {tokenizer.vocab_size}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_STEPS = 100\n",
    "GRADIENT_CLIP = 1.0\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for notebook compatibility\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        input_features = batch['input_features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(\n",
    "            input_features=input_features,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'avg_loss': f\"{total_loss/(batch_idx+1):.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\\n\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, scheduler, device, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Average Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on a sample\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_batch = {\n",
    "        'input_features': sample['input_features'].unsqueeze(0).to(device),\n",
    "        'labels': sample['labels'].unsqueeze(0).to(device),\n",
    "        'attention_mask': sample['attention_mask'].unsqueeze(0).to(device),\n",
    "    }\n",
    "    \n",
    "    loss, logits = model(**sample_batch)\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    \n",
    "print(f\"Evaluation loss: {loss.item():.4f}\")\n",
    "print(f\"\\nPredicted tokens (first 20): {predictions[0, :20].cpu().tolist()}\")\n",
    "print(f\"Ground truth tokens (first 20): {sample['labels'][1:21].tolist()}\")\n",
    "\n",
    "# Decode predictions\n",
    "pred_events = tokenizer.decode(predictions[0].cpu().tolist())\n",
    "true_events = tokenizer.decode(sample['labels'].tolist())\n",
    "\n",
    "print(f\"\\nPredicted events (first 10):\")\n",
    "for i, event in enumerate(pred_events[:10]):\n",
    "    print(f\"  {i}: {event}\")\n",
    "\n",
    "print(f\"\\nGround truth events (first 10):\")\n",
    "for i, event in enumerate(true_events[:10]):\n",
    "    print(f\"  {i}: {event}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint_path = \"whisper_keyboard_model.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "# Save tokenizer config\n",
    "tokenizer_config = {\n",
    "    'codes': tokenizer.codes,\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "}\n",
    "\n",
    "with open('tokenizer_config.json', 'w') as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "\n",
    "print(\"Tokenizer config saved to tokenizer_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
