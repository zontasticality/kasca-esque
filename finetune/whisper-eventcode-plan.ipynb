{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper keystroke fine-tuning blueprint\n",
    "\n",
    "This notebook turns the earlier prose outline into an executable Colab-style plan. It keeps Whisper's encoder, trains a decoder whose tokens map **exactly** to observed `KeyboardEvent.code` values, and adds a reproducible data-ingest stage that syncs assets from the `/recordings` route before any modeling work begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasibility, risks, and objectives\n",
    "- Whisper's encoder already models short percussive events, so reusing it for keystrokes is realistic; the decoder is retrained from scratch to emit keyboard tokens instead of language text.\n",
    "- Expect to need hours of paired (audio, keylog) data; diversity across typists, hardware, and mic placements will decide accuracy.\n",
    "- Labels must stay perfectly ordered; we only rely on the sequence of `event.code` entries, so consistent capture timestamps from the web app remain critical.\n",
    "- We'll start with a small Whisper checkpoint (e.g., `tiny`/`small`) plus LoRA adapters so the encoder can adapt slightly without overfitting when data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition via `/recordings`\n",
    "1. The notebook discovers data by hitting the deployed SvelteKit `/recordings` route (same domain as the typing UI). That endpoint returns `{ audio, keylog }` pairs when both `.webm` and `.json` files exist.\n",
    "2. We mirror that list into a `recordings_cache/` folder that sits alongside this notebook. The sync logic:\n",
    "   - Fetches the remote manifest once per run.\n",
    "   - Downloads any missing files.\n",
    "   - Deletes local files that are no longer advertised by the server so the cache always matches `/recordings` exactly.\n",
    "3. Subsequent preprocessing (manifest building, token set discovery, etc.) always works off this synchronized cache, so there's a single source of truth and no stale artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2f49744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "# assert device == \"cuda\", \"Please enable GPU (e.g., Colab > Runtime > Change runtime type).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a709d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade --quiet \\\n",
    "    requests \\\n",
    "    \"transformers>=4.44.0\" \\\n",
    "    \"datasets>=2.19.0\" \\\n",
    "    \"accelerate>=0.33.0\" \\\n",
    "    \"peft>=0.12.0\" \\\n",
    "    \"soundfile\" \\\n",
    "    \"librosa\" \\\n",
    "    \"tokenizers>=0.15.0\" \\\n",
    "    \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b0b27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "import soundfile as sf\n",
    "\n",
    "from datasets import Audio, load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "CACHE_DIR = NOTEBOOK_DIR / \"recordings_cache\"\n",
    "MANIFEST_DIR = NOTEBOOK_DIR / \"manifests\"\n",
    "MANIFEST_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BASE_URL = os.environ.get(\"S5_BASE_URL\", \"http://kasca-esque.fly.dev\")\n",
    "LIST_ENDPOINT = \"/recordings\"\n",
    "DOWNLOAD_ENDPOINT = \"/recordings/\"\n",
    "TARGET_SAMPLING_RATE = 16_000\n",
    "BASE_MODEL_NAME = \"openai/whisper-small\"\n",
    "TRAIN_SPLIT = 0.9\n",
    "RNG = random.Random(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765767a6",
   "metadata": {},
   "source": [
    "### Sync `/recordings` into the local cache\n",
    "Set `S5_BASE_URL` (or edit `BASE_URL` above) so that `BASE_URL + /recordings` resolves to the deployed app. Run the cell whenever you want to refresh the cache; it keeps the folder contents identical to what the server currently exposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5b2e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synced 106 files into /home/zyansheep/Projects/kasca-esque/finetune/recordings_cache\n"
     ]
    }
   ],
   "source": [
    "def sync_recordings():\n",
    "    CACHE_DIR.mkdir(exist_ok=True)\n",
    "    list_url = urljoin(BASE_URL, LIST_ENDPOINT)\n",
    "    response = requests.get(list_url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    payload = response.json()\n",
    "    if isinstance(payload, dict) and \"error\" in payload:\n",
    "        raise RuntimeError(f\"Server error: {payload['error']}\")\n",
    "\n",
    "    # Build set of remote filenames and download missing ones\n",
    "    remote_files = set()\n",
    "    for item in payload:\n",
    "        for field in (\"audio\", \"keylog\"):\n",
    "            filename = item[field]\n",
    "            remote_files.add(filename)\n",
    "            dest = CACHE_DIR / filename\n",
    "            if not dest.exists():\n",
    "                file_url = urljoin(BASE_URL, f\"/recordings/{filename}\")\n",
    "                print(f\"Downloading {filename}...\")\n",
    "                with requests.get(file_url, stream=True, timeout=60) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(dest, \"wb\") as fh:\n",
    "                        for chunk in r.iter_content(chunk_size=1 << 16):\n",
    "                            fh.write(chunk)\n",
    "\n",
    "    # Delete local files that are no longer available upstream\n",
    "    for local_path in CACHE_DIR.iterdir():\n",
    "        if local_path.name not in remote_files:\n",
    "            print(f\"Removing {local_path.name} (not advertised by server)...\")\n",
    "            local_path.unlink()\n",
    "\n",
    "    print(f\"Synced {len(remote_files)} files into {CACHE_DIR}\")\n",
    "\n",
    "sync_recordings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09138462",
   "metadata": {},
   "source": [
    "### Build train/eval manifests from the synchronized cache\n",
    "Each `.json` file holds the ordered keystrokes for its sibling `.webm`. This utility normalizes timestamps (seconds offset from the first event), keeps only entries whose `event.code` exists in the downloaded data, and emits two JSONL manifests under `manifests/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8401c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 47 rows to /home/zyansheep/Projects/kasca-esque/finetune/manifests/train.jsonl\n",
      "Wrote 6 rows to /home/zyansheep/Projects/kasca-esque/finetune/manifests/eval.jsonl\n"
     ]
    }
   ],
   "source": [
    "def build_manifests():\n",
    "    entries: List[Dict] = []\n",
    "    for json_path in CACHE_DIR.glob(\"*.json\"):\n",
    "        audio_path = json_path.with_suffix(\".webm\")\n",
    "        if not audio_path.exists():\n",
    "            continue\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        keystrokes = data.get(\"keystrokes\") or data.get(\"events\") or []\n",
    "        if not keystrokes:\n",
    "            continue\n",
    "        start_ts = keystrokes[0][\"timestamp\"]\n",
    "        example_events = []\n",
    "        for event in keystrokes:\n",
    "            code = event.get(\"key\")\n",
    "            etype = event.get(\"event_type\")\n",
    "            if not code or not etype:\n",
    "                continue\n",
    "            example_events.append(\n",
    "                {\n",
    "                    \"time\": (event[\"timestamp\"] - start_ts) / 1000.0,\n",
    "                    \"code\": code,\n",
    "                    \"type\": \"down\" if etype == \"keydown\" else \"up\",\n",
    "                }\n",
    "            )\n",
    "        if not example_events:\n",
    "            continue\n",
    "        entries.append(\n",
    "            {\n",
    "                \"audio\": str(audio_path),\n",
    "                \"events\": example_events,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not entries:\n",
    "        raise RuntimeError(\"No paired recordings found in cache.\")\n",
    "\n",
    "    RNG.shuffle(entries)\n",
    "    split_idx = int(len(entries) * TRAIN_SPLIT)\n",
    "    train_entries = entries[:split_idx]\n",
    "    eval_entries = entries[split_idx:] or entries[-1:]\n",
    "\n",
    "    train_path = MANIFEST_DIR / \"train.jsonl\"\n",
    "    eval_path = MANIFEST_DIR / \"eval.jsonl\"\n",
    "\n",
    "    for path, subset in ((train_path, train_entries), (eval_path, eval_entries)):\n",
    "        with open(path, \"w\") as fh:\n",
    "            for row in subset:\n",
    "                fh.write(json.dumps(row) + \"\\n\")\n",
    "        print(f\"Wrote {len(subset)} rows to {path}\")\n",
    "\n",
    "build_manifests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive the exact `event.code` vocabulary from the cached logs\n",
    "We now recompute the vocabulary straight from the synchronized JSON files so that every decoder token corresponds to a real `event.code`. Up/down states are expressed by duplicating each code into `_DOWN` / `_UP` variants, but no other synthetic key names are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 62 distinct event.code values.\n",
      "['AltRight', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'ArrowUp', 'Backquote', 'Backspace', 'BracketLeft', 'BracketRight', 'CapsLock', 'Comma', 'ControlLeft', 'ControlRight', 'Delete', 'Digit0', 'Digit1', 'Digit2', 'Digit3', 'Digit4', 'Digit5', 'Digit6', 'Digit7', 'Digit8', 'Digit9', 'Enter', 'Equal', 'IntlBackslash', 'KeyA', 'KeyB', 'KeyC', 'KeyD', 'KeyE', 'KeyF', 'KeyG', 'KeyH', 'KeyI', 'KeyJ', 'KeyK', 'KeyL', 'KeyM', 'KeyN', 'KeyO', 'KeyP', 'KeyQ', 'KeyR', 'KeyS', 'KeyT', 'KeyU', 'KeyV', 'KeyW', 'KeyX', 'KeyY', 'KeyZ', 'MetaLeft', 'Minus', 'Period', 'Quote', 'Semicolon', 'ShiftLeft', 'ShiftRight', 'Slash', 'Space']\n",
      "Vocabulary size: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/zyansheep/Projects/kasca-esque/finetune/key_tokenizer/tokenizer_config.json',\n",
       " '/home/zyansheep/Projects/kasca-esque/finetune/key_tokenizer/special_tokens_map.json',\n",
       " '/home/zyansheep/Projects/kasca-esque/finetune/key_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collect_event_codes(recordings_dir: Path) -> List[str]:\n",
    "    codes = set()\n",
    "    for json_path in recordings_dir.glob(\"*.json\"):\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        keystrokes = data.get(\"keystrokes\") or data.get(\"events\") or []\n",
    "        for event in keystrokes:\n",
    "            code = event.get(\"key\")\n",
    "            if code:\n",
    "                codes.add(code)\n",
    "    if not codes:\n",
    "        raise RuntimeError(\"No event.code entries found; double-check the cache.\")\n",
    "    return sorted(codes)\n",
    "\n",
    "EVENT_CODES = collect_event_codes(CACHE_DIR)\n",
    "print(f\"Discovered {len(EVENT_CODES)} distinct event.code values.\")\n",
    "print(EVENT_CODES)\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
    "event_tokens = [\n",
    "    f\"{code}_{suffix}\"\n",
    "    for code in EVENT_CODES\n",
    "    for suffix in (\"DOWN\", \"UP\")\n",
    "]\n",
    "\n",
    "vocab = {tok: idx for idx, tok in enumerate(special_tokens + event_tokens)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "backend_tokenizer = Tokenizer(WordLevel(vocab=vocab, unk_token=\"<unk>\"))\n",
    "backend_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=backend_tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(NOTEBOOK_DIR / \"key_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Whisper encoder, swap in the new decoder + tokenizer\n",
    "We clone the base Whisper config, override `vocab_size`, and copy only the encoder weights. Language/task forcing is disabled because we're no longer predicting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with pretrained encoder + fresh decoder.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "new_config = WhisperConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=base_model.config.d_model,\n",
    "    encoder_layers=base_model.config.encoder_layers,\n",
    "    encoder_attention_heads=base_model.config.encoder_attention_heads,\n",
    "    decoder_layers=base_model.config.decoder_layers,\n",
    "    decoder_attention_heads=base_model.config.decoder_attention_heads,\n",
    "    decoder_ffn_dim=base_model.config.decoder_ffn_dim,\n",
    "    encoder_ffn_dim=base_model.config.encoder_ffn_dim,\n",
    "    max_source_positions=base_model.config.max_source_positions,\n",
    "    max_target_positions=base_model.config.max_target_positions,\n",
    "    dropout=base_model.config.dropout,\n",
    "    attention_dropout=base_model.config.attention_dropout,\n",
    "    activation_dropout=base_model.config.activation_dropout,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "new_config.forced_decoder_ids = None\n",
    "new_config.suppress_tokens = []\n",
    "\n",
    "model = WhisperForConditionalGeneration(new_config)\n",
    "model.model.encoder.load_state_dict(base_model.model.encoder.state_dict())\n",
    "# Note: We can't use WhisperProcessor with a custom tokenizer, so we'll use feature_extractor and tokenizer separately\n",
    "model.to(device)\n",
    "print(\"Model initialized with pretrained encoder + fresh decoder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) LoRA adapters on the encoder\n",
    "Adapters let us nudge the encoder toward keyboard acoustics without updating every weight. Disable this if you prefer to keep the encoder frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 203,770,368 || trainable%: 0.8684\n"
     ]
    }
   ],
   "source": [
    "use_lora = True\n",
    "if use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"LoRA disabled; the encoder weights remain as-is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the base encoder weights (except LoRA) so we mainly train the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1769472 / 203770368 (0.87%)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"model.encoder\") and \"lora_\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total} ({trainable/total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the manifests and attach audio/label processing\n",
    "We leverage Hugging Face `datasets` to read the JSONL manifests, resample audio to 16 kHz, and convert each event list into a whitespace-separated token string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 47 examples [00:00, 866.34 examples/s]\n",
      "Generating validation split: 6 examples [00:00, 903.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'events'],\n",
      "        num_rows: 47\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'events'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": str(MANIFEST_DIR / \"train.jsonl\"),\n",
    "    \"validation\": str(MANIFEST_DIR / \"eval.jsonl\"),\n",
    "}\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files=data_files)\n",
    "datasets = datasets.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 47/47 [00:00<00:00, 57.87 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 54.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShiftLeft_DOWN ShiftLeft_DOWN ShiftLeft_DOWN ShiftLeft_DOWN ShiftLeft_DOWN ShiftLeft_DOWN KeyH_DOWN KeyH_UP ShiftLeft_UP\n"
     ]
    }
   ],
   "source": [
    "def events_to_text(example):\n",
    "    ordered = sorted(example[\"events\"], key=lambda e: e[\"time\"])\n",
    "    tokens = []\n",
    "    for evt in ordered:\n",
    "        code = evt[\"code\"]\n",
    "        state = evt[\"type\"].upper()\n",
    "        token = f\"{code}_{state}\"\n",
    "        if token not in vocab:\n",
    "            token = \"<unk>\"\n",
    "        tokens.append(token)\n",
    "    example[\"labels_text\"] = \" \".join(tokens)\n",
    "    return example\n",
    "\n",
    "textualized = datasets.map(events_to_text)\n",
    "print(textualized[\"train\"][0][\"labels_text\"][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/47 [01:48<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/datasets/utils/py_utils.py:612\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/multiprocess/managers.py:831\u001b[39m, in \u001b[36mBaseProxy._callmethod\u001b[39m\u001b[34m(self, methodname, args, kwds)\u001b[39m\n\u001b[32m    830\u001b[39m conn.send((\u001b[38;5;28mself\u001b[39m._id, methodname, args, kwds))\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m kind, result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kind == \u001b[33m'\u001b[39m\u001b[33m#RETURN\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/multiprocess/connection.py:253\u001b[39m, in \u001b[36m_ConnectionBase.recv\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler.loads(buf.getbuffer())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/multiprocess/connection.py:433\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/multiprocess/connection.py:398\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     chunk = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = labels\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m vectorized = \u001b[43mtextualized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprepare_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtextualized\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(vectorized)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/datasets/dataset_dict.py:953\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    951\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    975\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:3332\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3329\u001b[39m os.environ = prev_env\n\u001b[32m   3330\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3332\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43munprocessed_kwargs_per_job\u001b[49m\n\u001b[32m   3334\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3337\u001b[39m pool.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/datasets/utils/py_utils.py:626\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/multiprocess/pool.py:770\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    768\u001b[39m \u001b[38;5;28mself\u001b[39m.wait(timeout)\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ready():\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._success:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mTimeoutError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def prepare_example(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    inputs = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    labels = tokenizer(batch[\"labels_text\"], add_special_tokens=True).input_ids\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "vectorized = textualized.map(\n",
    "    prepare_example,\n",
    "    remove_columns=textualized[\"train\"].column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2Seq:\n",
    "    feature_extractor: WhisperFeatureExtractor\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_feats = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.feature_extractor.pad(input_feats, return_tensors=\"pt\")\n",
    "        label_feats = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.tokenizer.pad(label_feats, padding=True, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = DataCollatorSpeechSeq2Seq(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3501fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    labels = labels.copy()\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    pred_ids = np.argmax(preds, axis=-1)\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    exact = sum(p == l for p, l in zip(pred_str, label_str))\n",
    "    return {\"sequence_accuracy\": exact / len(pred_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465625af",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = NOTEBOOK_DIR / \"whisper_eventcode_lora\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    fp16=device == \"cuda\",  # Only use fp16 on GPU\n",
    "    predict_with_generate=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vectorized[\"train\"],\n",
    "    eval_dataset=vectorized[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dad75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = NOTEBOOK_DIR / \"whisper_eventcode_artifacts\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir / \"tokenizer\")\n",
    "feature_extractor.save_pretrained(save_dir / \"feature_extractor\")\n",
    "print(f\"Artifacts stored under {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def decode_recording(wav_path: str, max_length: int = 256):\n",
    "    audio_array, sr = sf.read(wav_path)\n",
    "    if sr != TARGET_SAMPLING_RATE:\n",
    "        import librosa\n",
    "        audio_array = librosa.resample(y=audio_array, orig_sr=sr, target_sr=TARGET_SAMPLING_RATE)\n",
    "        sr = TARGET_SAMPLING_RATE\n",
    "    inputs = feature_extractor(audio_array, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_features,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "    return decoded.split()\n",
    "\n",
    "# Example usage (update path after training):\n",
    "# predicted_tokens = decode_recording(str(CACHE_DIR / \"example.webm\"))\n",
    "# print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Expand the dataset (more typists, keyboards, and mics) so the decoder sees diverse acoustics.\n",
    "- Experiment with different Whisper checkpoints (`tiny`, `base`, `small`) and LoRA ranks to balance speed vs. accuracy.\n",
    "- Add richer evaluation metrics (per-token F1, timing alignment) or streaming decoding logic once the offline model is reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
