{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper keystroke fine-tuning blueprint\n",
    "\n",
    "This notebook turns the earlier prose outline into an executable Colab-style plan. It keeps Whisper's encoder, trains a decoder whose tokens map **exactly** to observed `KeyboardEvent.code` values, and adds a reproducible data-ingest stage that syncs assets from the `/recordings` route before any modeling work begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasibility, risks, and objectives\n",
    "- Whisper's encoder already models short percussive events, so reusing it for keystrokes is realistic; the decoder is retrained from scratch to emit keyboard tokens instead of language text.\n",
    "- Expect to need hours of paired (audio, keylog) data; diversity across typists, hardware, and mic placements will decide accuracy.\n",
    "- Labels must stay perfectly ordered; we only rely on the sequence of `event.code` entries, so consistent capture timestamps from the web app remain critical.\n",
    "- We'll start with a small Whisper checkpoint (e.g., `tiny`/`small`) plus LoRA adapters so the encoder can adapt slightly without overfitting when data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition via `/recordings`\n",
    "1. The notebook discovers data by hitting the deployed SvelteKit `/recordings` route (same domain as the typing UI). That endpoint returns `{ audio, keylog }` pairs when both `.webm` and `.json` files exist.\n",
    "2. We mirror that list into a `recordings_cache/` folder that sits alongside this notebook. The sync logic:\n",
    "   - Fetches the remote manifest once per run.\n",
    "   - Downloads any missing files.\n",
    "   - Deletes local files that are no longer advertised by the server so the cache always matches `/recordings` exactly.\n",
    "3. Subsequent preprocessing (manifest building, token set discovery, etc.) always works off this synchronized cache, so there's a single source of truth and no stale artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f49744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "# assert device == \"cuda\", \"Please enable GPU (e.g., Colab > Runtime > Change runtime type).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a709d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade --quiet \\\n",
    "    requests \\\n",
    "    \"transformers>=4.44.0\" \\\n",
    "    \"datasets>=2.19.0\" \\\n",
    "    \"accelerate>=0.33.0\" \\\n",
    "    \"peft>=0.12.0\" \\\n",
    "    \"soundfile\" \\\n",
    "    \"librosa\" \\\n",
    "    \"tokenizers>=0.15.0\" \\\n",
    "    \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0b27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "import soundfile as sf\n",
    "\n",
    "from datasets import Audio, load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "CACHE_DIR = NOTEBOOK_DIR / \"recordings_cache\"\n",
    "MANIFEST_DIR = NOTEBOOK_DIR / \"manifests\"\n",
    "MANIFEST_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BASE_URL = os.environ.get(\"S5_BASE_URL\", \"http://localhost:3000\")\n",
    "LIST_ENDPOINT = \"/recordings\"\n",
    "DOWNLOAD_ENDPOINT = \"/recordings/\"\n",
    "TARGET_SAMPLING_RATE = 16_000\n",
    "BASE_MODEL_NAME = \"openai/whisper-small\"\n",
    "TRAIN_SPLIT = 0.9\n",
    "RNG = random.Random(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765767a6",
   "metadata": {},
   "source": [
    "### Sync `/recordings` into the local cache\n",
    "Set `S5_BASE_URL` (or edit `BASE_URL` above) so that `BASE_URL + /recordings` resolves to the deployed app. Run the cell whenever you want to refresh the cache; it keeps the folder contents identical to what the server currently exposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b2e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading qt_1746047498719_email_1.webm...\n",
      "Downloading qt_1746047498719_email_1.json...\n",
      "Downloading qt_1746895943943_email_1.webm...\n",
      "Downloading qt_1746895943943_email_1.json...\n",
      "Downloading qt_1746895943943_free_form.webm...\n",
      "Downloading qt_1746895943943_free_form.json...\n",
      "Downloading qt_1748184742062_email_1.webm...\n",
      "Downloading qt_1748184742062_email_1.json...\n",
      "Downloading qt_1748184742062_free_form.webm...\n",
      "Downloading qt_1748184742062_free_form.json...\n",
      "Downloading qt_1749404117992_email_1.webm...\n",
      "Downloading qt_1749404117992_email_1.json...\n",
      "Downloading qt_1749404117992_free_form.webm...\n",
      "Downloading qt_1749404117992_free_form.json...\n",
      "Downloading qt_1749405175802_email_1.webm...\n",
      "Downloading qt_1749405175802_email_1.json...\n",
      "Downloading qt_1749405175802_free_form.webm...\n",
      "Downloading qt_1749405175802_free_form.json...\n",
      "Downloading qt_1749408901065_email_1.webm...\n",
      "Downloading qt_1749408901065_email_1.json...\n",
      "Downloading qt_1749408901065_free_form.webm...\n",
      "Downloading qt_1749408901065_free_form.json...\n",
      "Downloading qt_1749761963291_email_1.webm...\n",
      "Downloading qt_1749761963291_email_1.json...\n",
      "Downloading qt_1749761963291_free_form.webm...\n",
      "Downloading qt_1749761963291_free_form.json...\n",
      "Downloading qt_1749848552186_email_1.webm...\n",
      "Downloading qt_1749848552186_email_1.json...\n",
      "Downloading qt_1749848552186_free_form.webm...\n",
      "Downloading qt_1749848552186_free_form.json...\n",
      "Downloading qt_1749917131651_email_1.webm...\n",
      "Downloading qt_1749917131651_email_1.json...\n",
      "Downloading qt_1749917131651_free_form.webm...\n",
      "Downloading qt_1749917131651_free_form.json...\n",
      "Downloading qt_1749918170369_email_1.webm...\n",
      "Downloading qt_1749918170369_email_1.json...\n",
      "Downloading qt_1749918170369_free_form.webm...\n",
      "Downloading qt_1749918170369_free_form.json...\n",
      "Downloading qt_1749929413209_email_1.webm...\n",
      "Downloading qt_1749929413209_email_1.json...\n",
      "Downloading qt_1749929413209_free_form.webm...\n",
      "Downloading qt_1749929413209_free_form.json...\n",
      "Downloading qt_1749931434873_email_1.webm...\n",
      "Downloading qt_1749931434873_email_1.json...\n",
      "Downloading qt_1749931434873_free_form.webm...\n",
      "Downloading qt_1749931434873_free_form.json...\n",
      "Downloading qt_1750017190705_email_1.webm...\n",
      "Downloading qt_1750017190705_email_1.json...\n",
      "Downloading qt_1750017190705_free_form.webm...\n",
      "Downloading qt_1750017190705_free_form.json...\n",
      "Downloading qt_1750017896391_email_1.webm...\n",
      "Downloading qt_1750017896391_email_1.json...\n",
      "Downloading qt_1750017896391_free_form.webm...\n",
      "Downloading qt_1750017896391_free_form.json...\n",
      "Downloading qt_1752430034157_email_1.webm...\n",
      "Downloading qt_1752430034157_email_1.json...\n",
      "Downloading qt_1752430034157_free_form.webm...\n",
      "Downloading qt_1752430034157_free_form.json...\n",
      "Downloading qt_1752447148692_email_1.webm...\n",
      "Downloading qt_1752447148692_email_1.json...\n",
      "Downloading qt_1752447148692_free_form.webm...\n",
      "Downloading qt_1752447148692_free_form.json...\n",
      "Downloading qt_1753275076740_email_1.webm...\n",
      "Downloading qt_1753275076740_email_1.json...\n",
      "Downloading qt_1753275076740_free_form.webm...\n",
      "Downloading qt_1753275076740_free_form.json...\n",
      "Downloading qt_1753281609950_email_1.webm...\n",
      "Downloading qt_1753281609950_email_1.json...\n",
      "Downloading qt_1753281609950_free_form.webm...\n",
      "Downloading qt_1753281609950_free_form.json...\n",
      "Downloading qt_1753283592735_email_1.webm...\n",
      "Downloading qt_1753283592735_email_1.json...\n",
      "Downloading qt_1753283592735_free_form.webm...\n",
      "Downloading qt_1753283592735_free_form.json...\n",
      "Downloading qt_1753290632279_email_1.webm...\n",
      "Downloading qt_1753290632279_email_1.json...\n",
      "Downloading qt_1753290632279_free_form.webm...\n",
      "Downloading qt_1753290632279_free_form.json...\n",
      "Downloading qt_1753457979865_email_1.webm...\n",
      "Downloading qt_1753457979865_email_1.json...\n",
      "Downloading qt_1753457979865_free_form.webm...\n",
      "Downloading qt_1753457979865_free_form.json...\n",
      "Downloading qt_1753460583065_email_1.webm...\n",
      "Downloading qt_1753460583065_email_1.json...\n",
      "Downloading qt_1753460583065_free_form.webm...\n",
      "Downloading qt_1753460583065_free_form.json...\n",
      "Downloading qt_1753464926271_email_1.webm...\n",
      "Downloading qt_1753464926271_email_1.json...\n",
      "Downloading qt_1753464926271_free_form.webm...\n",
      "Downloading qt_1753464926271_free_form.json...\n",
      "Downloading qt_1753467140611_email_1.webm...\n",
      "Downloading qt_1753467140611_email_1.json...\n",
      "Downloading qt_1753467140611_free_form.webm...\n",
      "Downloading qt_1753467140611_free_form.json...\n",
      "Downloading qt_1753473543311_email_1.webm...\n",
      "Downloading qt_1753473543311_email_1.json...\n",
      "Downloading qt_1753473543311_free_form.webm...\n",
      "Downloading qt_1753473543311_free_form.json...\n",
      "Downloading qt_1753476550135_email_1.webm...\n",
      "Downloading qt_1753476550135_email_1.json...\n",
      "Downloading qt_1753476550135_free_form.webm...\n",
      "Downloading qt_1753476550135_free_form.json...\n",
      "Downloading recording_20251108_150818.webm...\n",
      "Downloading recording_20251108_150818.json...\n",
      "Downloading recording_20251108_151823.webm...\n",
      "Downloading recording_20251108_151823.json...\n",
      "Downloading recording_20251108_152117.webm...\n",
      "Downloading recording_20251108_152117.json...\n",
      "Downloading recording_20251108_155148.webm...\n",
      "Downloading recording_20251108_155148.json...\n",
      "Downloading recording_20251108_155510.webm...\n",
      "Downloading recording_20251108_155510.json...\n",
      "Downloading recording_20251108_155618.webm...\n",
      "Downloading recording_20251108_155618.json...\n",
      "Downloading recording_20251108_163605.webm...\n",
      "Downloading recording_20251108_163605.json...\n",
      "Downloading recording_20251108_163618.webm...\n",
      "Downloading recording_20251108_163618.json...\n",
      "Downloading recording_20251108_211606.webm...\n",
      "Downloading recording_20251108_211606.json...\n",
      "Downloading recording_20251108_221538.webm...\n",
      "Downloading recording_20251108_221538.json...\n",
      "Downloading recording_20251108_221934.webm...\n",
      "Downloading recording_20251108_221934.json...\n",
      "Synced 124 files into /home/zyansheep/Projects/kasca-esque/finetune/recordings_cache\n"
     ]
    }
   ],
   "source": [
    "def sync_recordings():\n",
    "    CACHE_DIR.mkdir(exist_ok=True)\n",
    "    list_url = urljoin(BASE_URL, LIST_ENDPOINT)\n",
    "    response = requests.get(list_url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    payload = response.json()\n",
    "    if isinstance(payload, dict) and \"error\" in payload:\n",
    "        raise RuntimeError(f\"Server error: {payload['error']}\")\n",
    "\n",
    "    # Build set of remote filenames and download missing ones\n",
    "    remote_files = set()\n",
    "    for item in payload:\n",
    "        for field in (\"audio\", \"keylog\"):\n",
    "            filename = item[field]\n",
    "            remote_files.add(filename)\n",
    "            dest = CACHE_DIR / filename\n",
    "            if not dest.exists():\n",
    "                file_url = urljoin(BASE_URL, f\"/recordings/{filename}\")\n",
    "                print(f\"Downloading {filename}...\")\n",
    "                with requests.get(file_url, stream=True, timeout=60) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(dest, \"wb\") as fh:\n",
    "                        for chunk in r.iter_content(chunk_size=1 << 16):\n",
    "                            fh.write(chunk)\n",
    "\n",
    "    # Delete local files that are no longer available upstream\n",
    "    for local_path in CACHE_DIR.iterdir():\n",
    "        if local_path.name not in remote_files:\n",
    "            print(f\"Removing {local_path.name} (not advertised by server)...\")\n",
    "            local_path.unlink()\n",
    "\n",
    "    print(f\"Synced {len(remote_files)} files into {CACHE_DIR}\")\n",
    "\n",
    "sync_recordings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09138462",
   "metadata": {},
   "source": [
    "### Build train/eval manifests from the synchronized cache\n",
    "Each `.json` file holds the ordered keystrokes for its sibling `.webm`. This utility normalizes timestamps (seconds offset from the first event), keeps only entries whose `event.code` exists in the downloaded data, and emits two JSONL manifests under `manifests/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8401c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 55 rows to /home/zyansheep/Projects/kasca-esque/finetune/manifests/train.jsonl\n",
      "Wrote 7 rows to /home/zyansheep/Projects/kasca-esque/finetune/manifests/eval.jsonl\n"
     ]
    }
   ],
   "source": [
    "def build_manifests():\n",
    "    entries: List[Dict] = []\n",
    "    for json_path in CACHE_DIR.glob(\"*.json\"):\n",
    "        audio_path = json_path.with_suffix(\".webm\")\n",
    "        if not audio_path.exists():\n",
    "            continue\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        keystrokes = data.get(\"keystrokes\") or data.get(\"events\") or []\n",
    "        if not keystrokes:\n",
    "            continue\n",
    "        start_ts = keystrokes[0][\"timestamp\"]\n",
    "        example_events = []\n",
    "        for event in keystrokes:\n",
    "            code = event.get(\"key\")\n",
    "            etype = event.get(\"event_type\")\n",
    "            if not code or not etype:\n",
    "                continue\n",
    "            example_events.append(\n",
    "                {\n",
    "                    \"time\": (event[\"timestamp\"] - start_ts) / 1000.0,\n",
    "                    \"code\": code,\n",
    "                    \"type\": \"down\" if etype == \"keydown\" else \"up\",\n",
    "                }\n",
    "            )\n",
    "        if not example_events:\n",
    "            continue\n",
    "        entries.append(\n",
    "            {\n",
    "                \"audio\": str(audio_path),\n",
    "                \"events\": example_events,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not entries:\n",
    "        raise RuntimeError(\"No paired recordings found in cache.\")\n",
    "\n",
    "    RNG.shuffle(entries)\n",
    "    split_idx = int(len(entries) * TRAIN_SPLIT)\n",
    "    train_entries = entries[:split_idx]\n",
    "    eval_entries = entries[split_idx:] or entries[-1:]\n",
    "\n",
    "    train_path = MANIFEST_DIR / \"train.jsonl\"\n",
    "    eval_path = MANIFEST_DIR / \"eval.jsonl\"\n",
    "\n",
    "    for path, subset in ((train_path, train_entries), (eval_path, eval_entries)):\n",
    "        with open(path, \"w\") as fh:\n",
    "            for row in subset:\n",
    "                fh.write(json.dumps(row) + \"\\n\")\n",
    "        print(f\"Wrote {len(subset)} rows to {path}\")\n",
    "\n",
    "build_manifests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive the exact `event.code` vocabulary from the cached logs\n",
    "We now recompute the vocabulary straight from the synchronized JSON files so that every decoder token corresponds to a real `event.code`. Up/down states are expressed by duplicating each code into `_DOWN` / `_UP` variants, but no other synthetic key names are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 62 distinct event.code values.\n",
      "['AltRight', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'ArrowUp', 'Backquote', 'Backspace', 'BracketLeft', 'BracketRight', 'CapsLock', 'Comma', 'ControlLeft', 'ControlRight', 'Delete', 'Digit0', 'Digit1', 'Digit2', 'Digit3', 'Digit4', 'Digit5', 'Digit6', 'Digit7', 'Digit8', 'Digit9', 'Enter', 'Equal', 'IntlBackslash', 'KeyA', 'KeyB', 'KeyC', 'KeyD', 'KeyE', 'KeyF', 'KeyG', 'KeyH', 'KeyI', 'KeyJ', 'KeyK', 'KeyL', 'KeyM', 'KeyN', 'KeyO', 'KeyP', 'KeyQ', 'KeyR', 'KeyS', 'KeyT', 'KeyU', 'KeyV', 'KeyW', 'KeyX', 'KeyY', 'KeyZ', 'MetaLeft', 'Minus', 'Period', 'Quote', 'Semicolon', 'ShiftLeft', 'ShiftRight', 'Slash', 'Space']\n",
      "Vocabulary size: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/zyansheep/Projects/kasca-esque/finetune/key_tokenizer/tokenizer_config.json',\n",
       " '/home/zyansheep/Projects/kasca-esque/finetune/key_tokenizer/special_tokens_map.json',\n",
       " '/home/zyansheep/Projects/kasca-esque/finetune/key_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collect_event_codes(recordings_dir: Path) -> List[str]:\n",
    "    codes = set()\n",
    "    for json_path in recordings_dir.glob(\"*.json\"):\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        keystrokes = data.get(\"keystrokes\") or data.get(\"events\") or []\n",
    "        for event in keystrokes:\n",
    "            code = event.get(\"key\")\n",
    "            if code:\n",
    "                codes.add(code)\n",
    "    if not codes:\n",
    "        raise RuntimeError(\"No event.code entries found; double-check the cache.\")\n",
    "    return sorted(codes)\n",
    "\n",
    "EVENT_CODES = collect_event_codes(CACHE_DIR)\n",
    "print(f\"Discovered {len(EVENT_CODES)} distinct event.code values.\")\n",
    "print(EVENT_CODES)\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
    "event_tokens = [\n",
    "    f\"{code}_{suffix}\"\n",
    "    for code in EVENT_CODES\n",
    "    for suffix in (\"DOWN\", \"UP\")\n",
    "]\n",
    "\n",
    "vocab = {tok: idx for idx, tok in enumerate(special_tokens + event_tokens)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "backend_tokenizer = Tokenizer(WordLevel(vocab=vocab, unk_token=\"<unk>\"))\n",
    "backend_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=backend_tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(NOTEBOOK_DIR / \"key_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Whisper encoder, swap in the new decoder + tokenizer\n",
    "We clone the base Whisper config, override `vocab_size`, and copy only the encoder weights. Language/task forcing is disabled because we're no longer predicting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Received a PreTrainedTokenizerFast for argument tokenizer, but a ('WhisperTokenizer', 'WhisperTokenizerFast') was expected.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m model = WhisperForConditionalGeneration(new_config)\n\u001b[32m     27\u001b[39m model.model.encoder.load_state_dict(base_model.model.encoder.state_dict())\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m processor = \u001b[43mWhisperProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m model.to(device)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel initialized with pretrained encoder + fresh decoder.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/transformers/models/whisper/processing_whisper.py:41\u001b[39m, in \u001b[36mWhisperProcessor.__init__\u001b[39m\u001b[34m(self, feature_extractor, tokenizer)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_extractor, tokenizer):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.current_processor = \u001b[38;5;28mself\u001b[39m.feature_extractor\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m._in_target_context_manager = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/transformers/processing_utils.py:534\u001b[39m, in \u001b[36mProcessorMixin.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Check each arg is of the proper class (this will also catch a user initializing in the wrong order)\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attribute_name, arg \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_argument_for_proper_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute_name, arg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/kasca-esque/.venv/lib/python3.13/site-packages/transformers/processing_utils.py:614\u001b[39m, in \u001b[36mProcessorMixin.check_argument_for_proper_class\u001b[39m\u001b[34m(self, argument_name, argument)\u001b[39m\n\u001b[32m    611\u001b[39m     proper_class = \u001b[38;5;28mself\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argument, proper_class):\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    615\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(argument).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m was expected.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    616\u001b[39m     )\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m proper_class\n",
      "\u001b[31mTypeError\u001b[39m: Received a PreTrainedTokenizerFast for argument tokenizer, but a ('WhisperTokenizer', 'WhisperTokenizerFast') was expected."
     ]
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "new_config = WhisperConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=base_model.config.d_model,\n",
    "    encoder_layers=base_model.config.encoder_layers,\n",
    "    encoder_attention_heads=base_model.config.encoder_attention_heads,\n",
    "    decoder_layers=base_model.config.decoder_layers,\n",
    "    decoder_attention_heads=base_model.config.decoder_attention_heads,\n",
    "    decoder_ffn_dim=base_model.config.decoder_ffn_dim,\n",
    "    encoder_ffn_dim=base_model.config.encoder_ffn_dim,\n",
    "    max_source_positions=base_model.config.max_source_positions,\n",
    "    max_target_positions=base_model.config.max_target_positions,\n",
    "    dropout=base_model.config.dropout,\n",
    "    attention_dropout=base_model.config.attention_dropout,\n",
    "    activation_dropout=base_model.config.activation_dropout,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "new_config.forced_decoder_ids = None\n",
    "new_config.suppress_tokens = []\n",
    "\n",
    "model = WhisperForConditionalGeneration(new_config)\n",
    "model.model.encoder.load_state_dict(base_model.model.encoder.state_dict())\n",
    "processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "model.to(device)\n",
    "print(\"Model initialized with pretrained encoder + fresh decoder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) LoRA adapters on the encoder\n",
    "Adapters let us nudge the encoder toward keyboard acoustics without updating every weight. Disable this if you prefer to keep the encoder frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lora = True\n",
    "if use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"LoRA disabled; the encoder weights remain as-is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the base encoder weights (except LoRA) so we mainly train the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"model.encoder\") and \"lora_\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total} ({trainable/total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the manifests and attach audio/label processing\n",
    "We leverage Hugging Face `datasets` to read the JSONL manifests, resample audio to 16 kHz, and convert each event list into a whitespace-separated token string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": str(MANIFEST_DIR / \"train.jsonl\"),\n",
    "    \"validation\": str(MANIFEST_DIR / \"eval.jsonl\"),\n",
    "}\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files=data_files)\n",
    "datasets = datasets.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_to_text(example):\n",
    "    ordered = sorted(example[\"events\"], key=lambda e: e[\"time\"])\n",
    "    tokens = []\n",
    "    for evt in ordered:\n",
    "        code = evt[\"code\"]\n",
    "        state = evt[\"type\"].upper()\n",
    "        token = f\"{code}_{state}\"\n",
    "        if token not in vocab:\n",
    "            token = \"<unk>\"\n",
    "        tokens.append(token)\n",
    "    example[\"labels_text\"] = \" \".join(tokens)\n",
    "    return example\n",
    "\n",
    "textualized = datasets.map(events_to_text)\n",
    "print(textualized[\"train\"][0][\"labels_text\"][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    inputs = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    labels = tokenizer(batch[\"labels_text\"], add_special_tokens=True).input_ids\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "vectorized = textualized.map(\n",
    "    prepare_example,\n",
    "    remove_columns=textualized[\"train\"].column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2Seq:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_feats = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_feats, return_tensors=\"pt\")\n",
    "        label_feats = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_feats, padding=True, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = DataCollatorSpeechSeq2Seq(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load as load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    pred_ids = np.argmax(preds, axis=-1)\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    exact = sum(p == l for p, l in zip(pred_str, label_str))\n",
    "    return {\"sequence_accuracy\": exact / len(pred_str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = NOTEBOOK_DIR / \"whisper_eventcode_lora\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    predict_with_generate=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vectorized[\"train\"],\n",
    "    eval_dataset=vectorized[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = NOTEBOOK_DIR / \"whisper_eventcode_artifacts\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir / \"tokenizer\")\n",
    "processor.save_pretrained(save_dir / \"processor\")\n",
    "print(f\"Artifacts stored under {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def decode_recording(wav_path: str, max_length: int = 256):\n",
    "    audio_array, sr = sf.read(wav_path)\n",
    "    if sr != TARGET_SAMPLING_RATE:\n",
    "        import librosa\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=sr, target_sr=TARGET_SAMPLING_RATE)\n",
    "        sr = TARGET_SAMPLING_RATE\n",
    "    inputs = processor.feature_extractor(audio_array, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_features,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "    return decoded.split()\n",
    "\n",
    "# Example usage (update path after training):\n",
    "# predicted_tokens = decode_recording(str(CACHE_DIR / \"example.webm\"))\n",
    "# print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Expand the dataset (more typists, keyboards, and mics) so the decoder sees diverse acoustics.\n",
    "- Experiment with different Whisper checkpoints (`tiny`, `base`, `small`) and LoRA ranks to balance speed vs. accuracy.\n",
    "- Add richer evaluation metrics (per-token F1, timing alignment) or streaming decoding logic once the offline model is reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
