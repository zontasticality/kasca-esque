{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper keystroke fine-tuning blueprint\n",
    "\n",
    "This notebook turns the earlier prose outline into an executable Colab-style plan. It keeps Whisper's encoder, trains a decoder whose tokens map **exactly** to observed `KeyboardEvent.code` values, and adds a reproducible data-ingest stage that syncs assets from the `/recordings` route before any modeling work begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasibility, risks, and objectives\n",
    "- Whisper's encoder already models short percussive events, so reusing it for keystrokes is realistic; the decoder is retrained from scratch to emit keyboard tokens instead of language text.\n",
    "- Expect to need hours of paired (audio, keylog) data; diversity across typists, hardware, and mic placements will decide accuracy.\n",
    "- Labels must stay perfectly ordered; we only rely on the sequence of `event.code` entries, so consistent capture timestamps from the web app remain critical.\n",
    "- We'll start with a small Whisper checkpoint (e.g., `tiny`/`small`) plus LoRA adapters so the encoder can adapt slightly without overfitting when data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition via `/recordings`\n",
    "1. The notebook discovers data by hitting the deployed SvelteKit `/recordings` route (same domain as the typing UI). That endpoint returns `{ audio, keylog }` pairs when both `.webm` and `.json` files exist.\n",
    "2. We mirror that list into a `recordings_cache/` folder that sits alongside this notebook. The sync logic:\n",
    "   - Fetches the remote manifest once per run.\n",
    "   - Downloads any missing files.\n",
    "   - Deletes local files that are no longer advertised by the server so the cache always matches `/recordings` exactly.\n",
    "3. Subsequent preprocessing (manifest building, token set discovery, etc.) always works off this synchronized cache, so there's a single source of truth and no stale artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "assert device == \"cuda\", \"Please enable GPU (e.g., Colab > Runtime > Change runtime type).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --upgrade --quiet \\\n",
    "    requests \\\n",
    "    \"transformers>=4.44.0\" \\\n",
    "    \"datasets>=2.19.0\" \\\n",
    "    \"accelerate>=0.33.0\" \\\n",
    "    \"peft>=0.12.0\" \\\n",
    "    \"soundfile\" \\\n",
    "    \"librosa\" \\\n",
    "    \"tokenizers>=0.15.0\" \\\n",
    "    \"evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "import soundfile as sf\n",
    "\n",
    "from datasets import Audio, load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    WhisperConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "CACHE_DIR = NOTEBOOK_DIR / \"recordings_cache\"\n",
    "MANIFEST_DIR = NOTEBOOK_DIR / \"manifests\"\n",
    "MANIFEST_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BASE_URL = os.environ.get(\"S5_BASE_URL\", \"http://kasca-esque.fly.dev\")\n",
    "LIST_ENDPOINT = \"/recordings\"\n",
    "DOWNLOAD_ENDPOINT = \"/recordings/\"\n",
    "TARGET_SAMPLING_RATE = 16_000\n",
    "BASE_MODEL_NAME = \"openai/whisper-small\"\n",
    "TRAIN_SPLIT = 0.9\n",
    "RNG = random.Random(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync `/recordings` into the local cache\n",
    "Set `S5_BASE_URL` (or edit `BASE_URL` above) so that `BASE_URL + /recordings` resolves to the deployed app. Run the cell whenever you want to refresh the cache; it keeps the folder contents identical to what the server currently exposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sync_recordings():\n",
    "    CACHE_DIR.mkdir(exist_ok=True)\n",
    "    list_url = urljoin(BASE_URL, LIST_ENDPOINT)\n",
    "    response = requests.get(list_url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    payload = response.json()\n",
    "    if isinstance(payload, dict) and \"error\" in payload:\n",
    "        raise RuntimeError(f\"Server error: {payload['error']}\")\n",
    "\n",
    "    # Build set of remote filenames and download missing ones\n",
    "    remote_files = set()\n",
    "    for item in payload:\n",
    "        for field in (\"audio\", \"keylog\"):\n",
    "            filename = item[field]\n",
    "            remote_files.add(filename)\n",
    "            dest = CACHE_DIR / filename\n",
    "            if not dest.exists():\n",
    "                file_url = urljoin(BASE_URL, f\"/recordings/{filename}\")\n",
    "                print(f\"Downloading {filename}...\")\n",
    "                with requests.get(file_url, stream=True, timeout=60) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(dest, \"wb\") as fh:\n",
    "                        for chunk in r.iter_content(chunk_size=1 << 16):\n",
    "                            fh.write(chunk)\n",
    "\n",
    "    # Delete local files that are no longer available upstream\n",
    "    for local_path in CACHE_DIR.iterdir():\n",
    "        if local_path.name not in remote_files:\n",
    "            print(f\"Removing {local_path.name} (not advertised by server)...\")\n",
    "            local_path.unlink()\n",
    "\n",
    "    print(f\"Synced {len(remote_files)} files into {CACHE_DIR}\")\n",
    "\n",
    "sync_recordings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train/eval manifests from the synchronized cache\n",
    "Each `.json` file holds the ordered keystrokes for its sibling `.webm`. This utility normalizes timestamps (seconds offset from the first event), keeps only entries whose `event.code` exists in the downloaded data, and emits two JSONL manifests under `manifests/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_manifests():\n",
    "    entries: List[Dict] = []\n",
    "    for json_path in CACHE_DIR.glob(\"*.json\"):\n",
    "        audio_path = json_path.with_suffix(\".webm\")\n",
    "        if not audio_path.exists():\n",
    "            continue\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        keystrokes = data.get(\"keystrokes\") or data.get(\"events\") or []\n",
    "        if not keystrokes:\n",
    "            continue\n",
    "        start_ts = keystrokes[0][\"timestamp\"]\n",
    "        example_events = []\n",
    "        for event in keystrokes:\n",
    "            code = event.get(\"key\")\n",
    "            etype = event.get(\"event_type\")\n",
    "            if not code or not etype:\n",
    "                continue\n",
    "            example_events.append(\n",
    "                {\n",
    "                    \"time\": (event[\"timestamp\"] - start_ts) / 1000.0,\n",
    "                    \"code\": code,\n",
    "                    \"type\": \"down\" if etype == \"keydown\" else \"up\",\n",
    "                }\n",
    "            )\n",
    "        if not example_events:\n",
    "            continue\n",
    "        entries.append(\n",
    "            {\n",
    "                \"audio\": str(audio_path),\n",
    "                \"events\": example_events,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not entries:\n",
    "        raise RuntimeError(\"No paired recordings found in cache.\")\n",
    "\n",
    "    RNG.shuffle(entries)\n",
    "    split_idx = int(len(entries) * TRAIN_SPLIT)\n",
    "    train_entries = entries[:split_idx]\n",
    "    eval_entries = entries[split_idx:] or entries[-1:]\n",
    "\n",
    "    train_path = MANIFEST_DIR / \"train.jsonl\"\n",
    "    eval_path = MANIFEST_DIR / \"eval.jsonl\"\n",
    "\n",
    "    for path, subset in ((train_path, train_entries), (eval_path, eval_entries)):\n",
    "        with open(path, \"w\") as fh:\n",
    "            for row in subset:\n",
    "                fh.write(json.dumps(row) + \"\\n\")\n",
    "        print(f\"Wrote {len(subset)} rows to {path}\")\n",
    "\n",
    "build_manifests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive the exact `event.code` vocabulary from the cached logs\n",
    "We now recompute the vocabulary straight from the synchronized JSON files so that every decoder token corresponds to a real `event.code`. Up/down states are expressed by duplicating each code into `_DOWN` / `_UP` variants, but no other synthetic key names are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359e1ae",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_event_codes(recordings_dir: Path) -> List[str]:\n",
    "    codes = set()\n",
    "    for json_path in recordings_dir.glob(\"*.json\"):\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        keystrokes = data.get(\"keystrokes\") or data.get(\"events\") or []\n",
    "        for event in keystrokes:\n",
    "            code = event.get(\"key\")\n",
    "            if code:\n",
    "                codes.add(code)\n",
    "    if not codes:\n",
    "        raise RuntimeError(\"No event.code entries found; double-check the cache.\")\n",
    "    return sorted(codes)\n",
    "\n",
    "EVENT_CODES = collect_event_codes(CACHE_DIR)\n",
    "print(f\"Discovered {len(EVENT_CODES)} distinct event.code values.\")\n",
    "print(EVENT_CODES)\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
    "event_tokens = [\n",
    "    f\"{code}_{suffix}\"\n",
    "    for code in EVENT_CODES\n",
    "    for suffix in (\"DOWN\", \"UP\")\n",
    "]\n",
    "\n",
    "vocab = {tok: idx for idx, tok in enumerate(special_tokens + event_tokens)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "backend_tokenizer = Tokenizer(WordLevel(vocab=vocab, unk_token=\"<unk>\"))\n",
    "backend_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=backend_tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "\n",
    "# Set padding side to right (default for most seq2seq models)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "tokenizer.save_pretrained(NOTEBOOK_DIR / \"key_tokenizer\")\n",
    "print(f\"Tokenizer padding side: {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4178e",
   "metadata": {},
   "source": [
    "### Load Whisper encoder, swap in the new decoder + tokenizer\n",
    "We clone the base Whisper config, override `vocab_size`, and copy only the encoder weights. Language/task forcing is disabled because we're no longer predicting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229bcdd",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(BASE_MODEL_NAME)\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "new_config = WhisperConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=base_model.config.d_model,\n",
    "    encoder_layers=base_model.config.encoder_layers,\n",
    "    encoder_attention_heads=base_model.config.encoder_attention_heads,\n",
    "    decoder_layers=base_model.config.decoder_layers,\n",
    "    decoder_attention_heads=base_model.config.decoder_attention_heads,\n",
    "    decoder_ffn_dim=base_model.config.decoder_ffn_dim,\n",
    "    encoder_ffn_dim=base_model.config.encoder_ffn_dim,\n",
    "    max_source_positions=base_model.config.max_source_positions,\n",
    "    max_target_positions=base_model.config.max_target_positions,\n",
    "    dropout=base_model.config.dropout,\n",
    "    attention_dropout=base_model.config.attention_dropout,\n",
    "    activation_dropout=base_model.config.activation_dropout,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,  # Important for generation\n",
    ")\n",
    "\n",
    "new_config.forced_decoder_ids = None\n",
    "new_config.suppress_tokens = []\n",
    "\n",
    "model = WhisperForConditionalGeneration(new_config)\n",
    "model.model.encoder.load_state_dict(base_model.model.encoder.state_dict())\n",
    "\n",
    "# Free up memory by deleting the base model\n",
    "del base_model\n",
    "import gc\n",
    "gc.collect()\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Configure generation settings\n",
    "model.generation_config.max_length = 256\n",
    "model.generation_config.max_new_tokens = None  # Use max_length instead\n",
    "model.generation_config.num_beams = 1  # Greedy decoding for speed\n",
    "model.generation_config.do_sample = False\n",
    "model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "# Note: We can't use WhisperProcessor with a custom tokenizer, so we'll use feature_extractor and tokenizer separately\n",
    "# Device placement happens after LoRA is applied (see cell below)\n",
    "print(\"Model initialized with pretrained encoder + fresh decoder.\")\n",
    "print(f\"Generation config: max_length={model.generation_config.max_length}, num_beams={model.generation_config.num_beams}\")\n",
    "print(f\"Decoder start token ID: {model.config.decoder_start_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd58ca",
   "metadata": {},
   "source": [
    "### (Optional) LoRA adapters on the encoder\n",
    "Adapters let us nudge the encoder toward keyboard acoustics without updating every weight. Disable this if you prefer to keep the encoder frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f47c00",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_lora = True\n",
    "if use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"LoRA disabled; the encoder weights remain as-is.\")\n",
    "\n",
    "# Move model to device after LoRA is applied\n",
    "model.to(device)\n",
    "print(f\"Model moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a11be1",
   "metadata": {},
   "source": [
    "### Freeze the base encoder weights (except LoRA) so we mainly train the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550f439",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"model.encoder\") and \"lora_\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Disable cache during training (required for gradient checkpointing and can cause issues)\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({trainable/total:.2%})\")\n",
    "print(f\"use_cache disabled for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79eefbf",
   "metadata": {},
   "source": [
    "### Load the manifests and attach audio/label processing\n",
    "We leverage Hugging Face `datasets` to read the JSONL manifests, resample audio to 16 kHz, and convert each event list into a whitespace-separated token string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cdd93",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": str(MANIFEST_DIR / \"train.jsonl\"),\n",
    "    \"validation\": str(MANIFEST_DIR / \"eval.jsonl\"),\n",
    "}\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files=data_files)\n",
    "datasets = datasets.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dca2ee",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def events_to_text(example):\n",
    "    ordered = sorted(example[\"events\"], key=lambda e: e[\"time\"])\n",
    "    tokens = []\n",
    "    for evt in ordered:\n",
    "        code = evt[\"code\"]\n",
    "        state = evt[\"type\"].upper()\n",
    "        token = f\"{code}_{state}\"\n",
    "        if token not in vocab:\n",
    "            token = \"<unk>\"\n",
    "        tokens.append(token)\n",
    "    example[\"labels_text\"] = \" \".join(tokens)\n",
    "    return example\n",
    "\n",
    "textualized = datasets.map(events_to_text)\n",
    "print(textualized[\"train\"][0][\"labels_text\"][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40803967",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_example(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    inputs = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    labels = tokenizer(batch[\"labels_text\"], add_special_tokens=True).input_ids\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "vectorized = textualized.map(\n",
    "    prepare_example,\n",
    "    remove_columns=textualized[\"train\"].column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1997e6",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2Seq:\n",
    "    feature_extractor: WhisperFeatureExtractor\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    "\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_feats = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.feature_extractor.pad(input_feats, return_tensors=\"pt\")\n",
    "        label_feats = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.tokenizer.pad(label_feats, padding=True, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = DataCollatorSpeechSeq2Seq(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f6aba",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    # When predict_with_generate=True, preds are already token IDs, not logits\n",
    "    # Replace -100 in labels with pad token for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate exact match accuracy\n",
    "    exact_matches = sum(p == l for p, l in zip(pred_str, label_str))\n",
    "    \n",
    "    # Also calculate token-level accuracy for more granular metrics\n",
    "    pred_tokens = [p.split() for p in pred_str]\n",
    "    label_tokens = [l.split() for l in label_str]\n",
    "    \n",
    "    correct_tokens = 0\n",
    "    total_tokens = 0\n",
    "    for p_toks, l_toks in zip(pred_tokens, label_tokens):\n",
    "        # Count matching tokens at each position\n",
    "        for i in range(min(len(p_toks), len(l_toks))):\n",
    "            if p_toks[i] == l_toks[i]:\n",
    "                correct_tokens += 1\n",
    "        total_tokens += max(len(p_toks), len(l_toks))\n",
    "    \n",
    "    return {\n",
    "        \"sequence_accuracy\": exact_matches / len(pred_str) if len(pred_str) > 0 else 0,\n",
    "        \"token_accuracy\": correct_tokens / total_tokens if total_tokens > 0 else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4b40c",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = NOTEBOOK_DIR / \"whisper_eventcode_lora\"\n",
    "\n",
    "# Calculate reasonable step counts based on dataset size\n",
    "# With 55 train samples, batch_size=4, grad_accum=4 -> ~3.4 steps/epoch -> ~34 total steps for 10 epochs\n",
    "steps_per_epoch = len(textualized[\"train\"]) // (4 * 4) + 1\n",
    "total_steps = steps_per_epoch * 10\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=min(10, total_steps // 4),  # Warmup for ~25% of training or 10 steps max\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate every epoch instead of by steps\n",
    "    save_strategy=\"epoch\",  # Save every epoch\n",
    "    logging_steps=max(1, steps_per_epoch // 2),  # Log twice per epoch\n",
    "    save_total_limit=3,\n",
    "    fp16=device == \"cuda\",  # Only use fp16 on GPU\n",
    "    predict_with_generate=True,  # Generate sequences during evaluation for proper metrics\n",
    "    generation_max_length=256,  # Max sequence length for generation\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"sequence_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    gradient_checkpointing=False,  # Set to True if running out of memory\n",
    "    optim=\"adamw_torch\",  # Use PyTorch's AdamW\n",
    ")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}, Total steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {training_args.warmup_steps}\")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vectorized[\"train\"],\n",
    "    eval_dataset=vectorized[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dir = NOTEBOOK_DIR / \"whisper_eventcode_artifacts\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir / \"tokenizer\")\n",
    "feature_extractor.save_pretrained(save_dir / \"feature_extractor\")\n",
    "print(f\"Artifacts stored under {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def decode_recording(wav_path: str, max_length: int = 256):\n",
    "    audio_array, sr = sf.read(wav_path)\n",
    "    if sr != TARGET_SAMPLING_RATE:\n",
    "        import librosa\n",
    "        audio_array = librosa.resample(y=audio_array, orig_sr=sr, target_sr=TARGET_SAMPLING_RATE)\n",
    "        sr = TARGET_SAMPLING_RATE\n",
    "    inputs = feature_extractor(audio_array, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_features,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "    return decoded.split()\n",
    "\n",
    "# Example usage (update path after training):\n",
    "# predicted_tokens = decode_recording(str(CACHE_DIR / \"example.webm\"))\n",
    "# print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Expand the dataset (more typists, keyboards, and mics) so the decoder sees diverse acoustics.\n",
    "- Experiment with different Whisper checkpoints (`tiny`, `base`, `small`) and LoRA ranks to balance speed vs. accuracy.\n",
    "- Add richer evaluation metrics (per-token F1, timing alignment) or streaming decoding logic once the offline model is reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
